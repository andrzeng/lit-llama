{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lit_llama import model\n",
    "import random\n",
    "from lit_llama import LLaMA, Tokenizer\n",
    "from lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fabric = L.Fabric(devices=1)\n",
    "tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('datasets/alpaca_data_cleaned.json') as f:\n",
    "    alpaca_json = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('datasets/alpaca_data_cleaned.json') as f:\n",
    "    alpaca_json = json.load(f)\n",
    "\n",
    "for item in alpaca_json:\n",
    "    if(len(item['input']) == 0):\n",
    "        print(item)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('datasets/alpaca_data_cleaned.json') as f:\n",
    "    alpaca_json = json.load(f)\n",
    "'''\n",
    "for item in alpaca_json:\n",
    "    if(len(item['input']) == 0):\n",
    "        item['input'] = ' '\n",
    "  \n",
    "\n",
    "'''\n",
    "\n",
    "# Create tokenized j\n",
    "squad_train = []\n",
    "squad_test = []\n",
    "\n",
    "for item in alpaca_json[:5176]:\n",
    "    if(len(item['input']) == 0):\n",
    "        squad_test.append(\n",
    "            {\n",
    "                'instruction': tokenizer.encode(item['instruction'], bos=True, eos=False, device=fabric.device),\n",
    "                'input': tokenizer.encode(item['input'], bos=False, eos=False, device=fabric.device),\n",
    "                'output':tokenizer.encode(item['output'], bos=False, eos=True, device=fabric.device)\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "for item in alpaca_json[5176:]:\n",
    "    if(len(item['input']) == 0):\n",
    "        squad_train.append(\n",
    "            {\n",
    "                'instruction': tokenizer.encode(item['instruction'], bos=True, eos=False, device=fabric.device),\n",
    "                'input': tokenizer.encode(item['input'], bos=False, eos=False, device=fabric.device),\n",
    "                'output':tokenizer.encode(item['output'], bos=False, eos=True, device=fabric.device)\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29515\n",
      "3088\n"
     ]
    }
   ],
   "source": [
    "print(len(squad_train))\n",
    "print(len(squad_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_example(dataset, index=None):\n",
    "    if(index is None):\n",
    "        index = random.sample(range(len(dataset)), k=1)[0]\n",
    "    # IST\n",
    "    IST = IST_generator(LLamaModel(dataset[index]['instruction'].unsqueeze(0).to(fabric.device))[1])[:,-1,:]\n",
    "\n",
    "    # Question\n",
    "    question = LLamaModel.transformer.wte(dataset[index]['input'].unsqueeze(0).to(fabric.device)).squeeze()\n",
    "\n",
    "    # Answer fragment\n",
    "    answer_len = dataset[index]['output'].size(0)\n",
    "    trunc_len = random.randint(0,answer_len-1)\n",
    "    #print(answer_len)\n",
    "    #print(trunc_len)\n",
    "\n",
    "    truncated_answer = dataset[index]['output'][:trunc_len]\n",
    "    truncated_answer = LLamaModel.transformer.wte(truncated_answer)\n",
    "    \n",
    "    target_tokens = torch.cat([dataset[index]['input'], dataset[index]['output'][:trunc_len+1]])\n",
    "    #print(tokenizer.decode(target_tokens))\n",
    "\n",
    "    if(question.dim() == 1):\n",
    "        question = question.unsqueeze(0)\n",
    "\n",
    "    if(truncated_answer.dim() == 1):\n",
    "        truncated_answer = truncated_answer.unsqueeze(0)\n",
    "\n",
    "    llama_input = torch.cat([IST,question,truncated_answer])\n",
    "    return llama_input.unsqueeze(0), target_tokens.type(torch.LongTensor).unsqueeze(0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\")\n",
    "tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "\n",
    "\n",
    "def load_LLaMA(checkpoint_path):\n",
    "    with lazy_load(checkpoint_path) as checkpoint:\n",
    "        name = llama_model_lookup(checkpoint)\n",
    "\n",
    "        with EmptyInitOnDevice(\n",
    "                device=fabric.device, dtype=dtype, quantization_mode=None # We won't quantize the weights\n",
    "        ):\n",
    "            model = LLaMA.from_name(name)\n",
    "\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Finished loading the first model\n",
      "Finished loading models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "\n",
    "LLaMA_config = model.LLaMAConfig.from_name('7B')\n",
    "print('Loading models...')\n",
    "# Load the LLaMa model and the IST generator (also a LLaMA model)\n",
    "LLamaModel = load_LLaMA(checkpoint_path).to(fabric.device)\n",
    "#LLamaModel = LLaMA(LLaMA_config).to(fabric.device)\n",
    "print('Finished loading the first model')\n",
    "print('Finished loading models')\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "IST_schemes = ['vanilla', 'last 4', '2nd to last', 'all layers']\n",
    "scheme_losses = {}\n",
    "\n",
    "IST_generator = model.Block(LLaMA_config)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(IST_generator.parameters(), lr=1e-4)\n",
    "IST_generator = IST_generator.to(fabric.device)\n",
    "\n",
    "for param in LLamaModel.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IST_generator = model.Block(LLaMA_config)\n",
    "IST_generator = IST_generator.to(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(IST_generator.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size=32\n",
    "trainset_size=4000\n",
    "testset_size=1000\n",
    "\n",
    "config = {\n",
    "    'lr': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'trainset_size': trainset_size,\n",
    "    'testset_size':testset_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrew-zeng\u001b[0m (\u001b[33msmalllanguagemodels\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrew/Documents/lit-llama/wandb/run-20230622_114131-14d4bgj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smalllanguagemodels/Alpaca%20instruction%20tuning/runs/14d4bgj6' target=\"_blank\">Training the IST generator on Alpaca samples with empty input</a></strong> to <a href='https://wandb.ai/smalllanguagemodels/Alpaca%20instruction%20tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smalllanguagemodels/Alpaca%20instruction%20tuning' target=\"_blank\">https://wandb.ai/smalllanguagemodels/Alpaca%20instruction%20tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smalllanguagemodels/Alpaca%20instruction%20tuning/runs/14d4bgj6' target=\"_blank\">https://wandb.ai/smalllanguagemodels/Alpaca%20instruction%20tuning/runs/14d4bgj6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/smalllanguagemodels/Alpaca%20instruction%20tuning/runs/14d4bgj6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd3b9fcebf0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init wandb\n",
    "wandb.init(\n",
    "    project='Alpaca instruction tuning',\n",
    "    config=config,\n",
    "    name=\"Training the IST generator on Alpaca samples with empty input\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(IST_generator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss = 5.7236328125, validation loss=2.375732421875\n",
      "epoch 0, train loss = 3.821533203125, validation loss=2.591064453125\n",
      "epoch 0, train loss = 2.6571044921875, validation loss=3.30908203125\n",
      "epoch 0, train loss = 2.62548828125, validation loss=3.2879638671875\n",
      "epoch 0, train loss = 2.8255615234375, validation loss=2.4464111328125\n",
      "epoch 0, train loss = 2.501708984375, validation loss=2.642333984375\n",
      "epoch 0, train loss = 2.522705078125, validation loss=2.10302734375\n",
      "epoch 0, train loss = 2.0941162109375, validation loss=2.79296875\n",
      "epoch 0, train loss = 2.6201171875, validation loss=2.7679443359375\n",
      "epoch 0, train loss = 2.658935546875, validation loss=2.4058837890625\n",
      "epoch 0, train loss = 2.185791015625, validation loss=2.31787109375\n",
      "epoch 0, train loss = 2.429443359375, validation loss=2.2755126953125\n",
      "epoch 0, train loss = 1.9488525390625, validation loss=2.341064453125\n",
      "epoch 0, train loss = 2.5023193359375, validation loss=2.442626953125\n",
      "epoch 0, train loss = 2.2188720703125, validation loss=2.555908203125\n",
      "epoch 0, train loss = 1.9033203125, validation loss=2.30712890625\n",
      "epoch 0, train loss = 2.135009765625, validation loss=2.007568359375\n",
      "epoch 0, train loss = 2.358642578125, validation loss=2.301025390625\n",
      "epoch 0, train loss = 2.18359375, validation loss=2.37841796875\n",
      "epoch 0, train loss = 2.263427734375, validation loss=2.7052001953125\n",
      "epoch 0, train loss = 2.5860595703125, validation loss=2.08544921875\n",
      "epoch 0, train loss = 1.994384765625, validation loss=2.4307861328125\n",
      "epoch 0, train loss = 2.088623046875, validation loss=1.7882080078125\n",
      "epoch 0, train loss = 1.9261474609375, validation loss=2.1497802734375\n",
      "epoch 0, train loss = 2.0291748046875, validation loss=2.234375\n",
      "epoch 0, train loss = 2.201416015625, validation loss=2.2044677734375\n",
      "epoch 0, train loss = 2.10693359375, validation loss=2.1324462890625\n",
      "epoch 0, train loss = 2.08038330078125, validation loss=2.332275390625\n",
      "epoch 0, train loss = 1.97265625, validation loss=1.9822998046875\n",
      "epoch 0, train loss = 1.959716796875, validation loss=2.1138916015625\n",
      "epoch 0, train loss = 2.4525146484375, validation loss=1.9962158203125\n",
      "epoch 0, train loss = 1.892578125, validation loss=1.922607421875\n",
      "epoch 0, train loss = 2.548095703125, validation loss=2.123779296875\n",
      "epoch 0, train loss = 1.928955078125, validation loss=2.373779296875\n",
      "epoch 0, train loss = 2.256103515625, validation loss=1.7579345703125\n",
      "epoch 0, train loss = 2.2283935546875, validation loss=2.0526123046875\n",
      "epoch 0, train loss = 2.001220703125, validation loss=2.32373046875\n",
      "epoch 0, train loss = 2.756591796875, validation loss=2.00244140625\n",
      "epoch 0, train loss = 1.856689453125, validation loss=2.4442138671875\n",
      "epoch 0, train loss = 2.1978759765625, validation loss=1.7744140625\n",
      "epoch 0, train loss = 1.990478515625, validation loss=2.0849609375\n",
      "epoch 0, train loss = 2.0654296875, validation loss=2.0657958984375\n",
      "epoch 0, train loss = 1.52978515625, validation loss=2.478759765625\n",
      "epoch 0, train loss = 2.0635986328125, validation loss=2.33642578125\n",
      "epoch 0, train loss = 2.205078125, validation loss=2.096435546875\n",
      "epoch 0, train loss = 2.008544921875, validation loss=1.7769775390625\n",
      "epoch 0, train loss = 2.03887939453125, validation loss=2.0015869140625\n",
      "epoch 0, train loss = 2.2474365234375, validation loss=1.7059326171875\n",
      "epoch 0, train loss = 1.7882080078125, validation loss=1.9259033203125\n",
      "epoch 0, train loss = 1.6885986328125, validation loss=1.896728515625\n",
      "epoch 0, train loss = 1.6424560546875, validation loss=1.810302734375\n",
      "epoch 0, train loss = 2.2161865234375, validation loss=1.6888427734375\n",
      "epoch 0, train loss = 2.0228271484375, validation loss=1.9580078125\n",
      "epoch 0, train loss = 1.974853515625, validation loss=2.10205078125\n",
      "epoch 0, train loss = 2.139892578125, validation loss=2.088134765625\n",
      "epoch 0, train loss = 1.669189453125, validation loss=2.124267578125\n",
      "epoch 0, train loss = 1.9429931640625, validation loss=2.1685791015625\n",
      "epoch 0, train loss = 2.1781005859375, validation loss=1.5972900390625\n",
      "epoch 0, train loss = 2.0693359375, validation loss=1.746337890625\n",
      "epoch 0, train loss = 1.8631591796875, validation loss=1.5675048828125\n",
      "epoch 0, train loss = 2.212890625, validation loss=2.269287109375\n",
      "epoch 0, train loss = 1.6143798828125, validation loss=1.4715576171875\n",
      "epoch 0, train loss = 1.9168701171875, validation loss=1.892822265625\n",
      "epoch 0, train loss = 2.158203125, validation loss=1.833740234375\n",
      "epoch 0, train loss = 1.986328125, validation loss=1.7406005859375\n",
      "epoch 0, train loss = 1.8131103515625, validation loss=2.060302734375\n",
      "epoch 0, train loss = 2.0703125, validation loss=1.7431640625\n",
      "epoch 0, train loss = 2.362060546875, validation loss=1.812744140625\n",
      "epoch 0, train loss = 1.71630859375, validation loss=1.8421630859375\n",
      "epoch 0, train loss = 1.929443359375, validation loss=1.83642578125\n",
      "epoch 0, train loss = 1.9073486328125, validation loss=1.765625\n",
      "epoch 0, train loss = 1.69384765625, validation loss=2.3665771484375\n",
      "epoch 0, train loss = 1.7906494140625, validation loss=2.0836181640625\n",
      "epoch 0, train loss = 1.5362548828125, validation loss=1.8856201171875\n",
      "epoch 0, train loss = 1.636962890625, validation loss=2.19140625\n",
      "epoch 0, train loss = 2.005859375, validation loss=1.965576171875\n",
      "epoch 0, train loss = 1.9315185546875, validation loss=1.94293212890625\n",
      "epoch 0, train loss = 1.8302001953125, validation loss=2.255615234375\n",
      "epoch 0, train loss = 1.7203369140625, validation loss=1.7091064453125\n",
      "epoch 0, train loss = 1.7081298828125, validation loss=1.646728515625\n",
      "epoch 0, train loss = 1.7203369140625, validation loss=1.7276611328125\n",
      "epoch 0, train loss = 1.7901611328125, validation loss=1.895751953125\n",
      "epoch 0, train loss = 2.021728515625, validation loss=1.92919921875\n",
      "epoch 0, train loss = 1.5670166015625, validation loss=1.9730224609375\n",
      "epoch 0, train loss = 1.789794921875, validation loss=1.76806640625\n",
      "epoch 0, train loss = 1.8525390625, validation loss=1.7813720703125\n",
      "epoch 0, train loss = 2.0965576171875, validation loss=1.7840576171875\n",
      "epoch 0, train loss = 1.8631591796875, validation loss=2.1187744140625\n",
      "epoch 0, train loss = 1.97021484375, validation loss=1.6378173828125\n",
      "epoch 0, train loss = 2.0428466796875, validation loss=2.1478271484375\n",
      "epoch 0, train loss = 1.6492919921875, validation loss=1.74591064453125\n",
      "epoch 0, train loss = 1.4658203125, validation loss=1.986572265625\n",
      "epoch 0, train loss = 1.8948974609375, validation loss=1.5633544921875\n",
      "epoch 0, train loss = 1.8687744140625, validation loss=1.39422607421875\n",
      "epoch 0, train loss = 2.0938720703125, validation loss=1.854248046875\n",
      "epoch 0, train loss = 1.8189697265625, validation loss=1.84912109375\n",
      "epoch 0, train loss = 2.08203125, validation loss=1.3834228515625\n",
      "epoch 0, train loss = 2.096923828125, validation loss=2.1846923828125\n",
      "epoch 0, train loss = 1.8300628662109375, validation loss=1.885986328125\n",
      "epoch 0, train loss = 1.804443359375, validation loss=2.0863037109375\n",
      "epoch 0, train loss = 2.1846923828125, validation loss=1.6746826171875\n",
      "epoch 0, train loss = 1.8819580078125, validation loss=1.846923828125\n",
      "epoch 0, train loss = 1.5699462890625, validation loss=1.7186279296875\n",
      "epoch 0, train loss = 1.90771484375, validation loss=1.8739013671875\n",
      "epoch 0, train loss = 1.9024658203125, validation loss=1.709716796875\n",
      "epoch 0, train loss = 1.7567138671875, validation loss=2.2698974609375\n",
      "epoch 0, train loss = 1.9761962890625, validation loss=2.2000732421875\n",
      "epoch 0, train loss = 1.9564208984375, validation loss=1.7911376953125\n",
      "epoch 0, train loss = 1.761474609375, validation loss=1.965087890625\n",
      "epoch 0, train loss = 1.843994140625, validation loss=1.463623046875\n",
      "epoch 0, train loss = 1.626953125, validation loss=1.7344970703125\n",
      "epoch 0, train loss = 1.87841796875, validation loss=1.87396240234375\n",
      "epoch 0, train loss = 2.3447265625, validation loss=1.5234375\n",
      "epoch 0, train loss = 1.5205078125, validation loss=1.5479736328125\n",
      "epoch 0, train loss = 2.6444091796875, validation loss=1.6357421875\n",
      "epoch 0, train loss = 1.718994140625, validation loss=1.642822265625\n",
      "epoch 0, train loss = 2.089599609375, validation loss=1.7669677734375\n",
      "epoch 0, train loss = 2.068115234375, validation loss=1.962158203125\n",
      "epoch 0, train loss = 1.9361572265625, validation loss=1.6221923828125\n",
      "epoch 0, train loss = 1.9388427734375, validation loss=1.97412109375\n",
      "epoch 0, train loss = 1.6676025390625, validation loss=1.5933837890625\n",
      "epoch 0, train loss = 1.9866943359375, validation loss=1.57763671875\n",
      "epoch 0, train loss = 2.0218505859375, validation loss=1.8663330078125\n",
      "epoch 0, train loss = 1.5577392578125, validation loss=1.8236083984375\n",
      "epoch 0, train loss = 1.5013427734375, validation loss=1.6883544921875\n",
      "epoch 0, train loss = 2.051513671875, validation loss=1.6612548828125\n",
      "epoch 0, train loss = 1.677490234375, validation loss=1.6993408203125\n",
      "epoch 0, train loss = 2.4376220703125, validation loss=1.8804931640625\n",
      "epoch 0, train loss = 1.921875, validation loss=1.642578125\n",
      "epoch 0, train loss = 1.658203125, validation loss=1.4923095703125\n",
      "epoch 0, train loss = 1.611328125, validation loss=1.7784423828125\n",
      "epoch 0, train loss = 1.644775390625, validation loss=1.3984375\n",
      "epoch 0, train loss = 1.6673583984375, validation loss=1.451171875\n",
      "epoch 0, train loss = 1.9910888671875, validation loss=1.6160888671875\n",
      "epoch 0, train loss = 1.635986328125, validation loss=1.789306640625\n",
      "epoch 0, train loss = 1.8302001953125, validation loss=1.6544189453125\n",
      "epoch 0, train loss = 1.6654052734375, validation loss=1.6004638671875\n",
      "epoch 0, train loss = 1.7991943359375, validation loss=1.8841552734375\n",
      "epoch 0, train loss = 1.635986328125, validation loss=1.94921875\n",
      "epoch 0, train loss = 2.005859375, validation loss=1.554443359375\n",
      "epoch 0, train loss = 1.824951171875, validation loss=1.3896484375\n",
      "epoch 0, train loss = 2.06634521484375, validation loss=1.6014404296875\n",
      "epoch 0, train loss = 2.168701171875, validation loss=1.4598388671875\n",
      "epoch 0, train loss = 1.83782958984375, validation loss=1.7056884765625\n",
      "epoch 0, train loss = 1.626708984375, validation loss=1.9873046875\n",
      "epoch 0, train loss = 1.57861328125, validation loss=1.7021484375\n",
      "epoch 0, train loss = 1.728759765625, validation loss=1.84814453125\n",
      "epoch 0, train loss = 1.7315673828125, validation loss=1.6953125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(batch_size):                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, target = get_single_example(squad_train, index=batch_indices[i])         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>llama_output = LLamaModel.forward_embeddings(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>.type(torch.bfloat16))[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch_loss += loss.item()                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">110</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward_embeddings</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = torch.cat((internal_state_tokens.reshape(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).to(embeddings.device),    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> block <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer.h:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>110 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = block(x)                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer.ln_f(x)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head(x)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (b, t, vocab_size)</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">138</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">136 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x: torch.Tensor) -&gt; torch.Tensor:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>138 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rms_1(x))                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rms_2(x))                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">140 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">198</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>device=x.device,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>198 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>q = apply_rope(q, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rope_cache)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>k = apply_rope(k, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rope_cache)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh,</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">292</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_rope</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">289 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># cast because the reference does</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>xshaped = x.float().reshape(*x.shape[:-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>rope_cache = rope_cache.view(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, xshaped.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, xshaped.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>292 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>x_out2 = torch.stack(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">293 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>[xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] - xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>],      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">294 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │    </span>xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] + xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>],      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">295 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>], -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(batch_size):                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m, target = get_single_example(squad_train, index=batch_indices[i])         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   │   \u001b[0mllama_output = LLamaModel.forward_embeddings(\u001b[96minput\u001b[0m.type(torch.bfloat16))[\u001b[94m0\u001b[0m]     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss.backward()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_loss += loss.item()                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m110\u001b[0m in \u001b[92mforward_embeddings\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = torch.cat((internal_state_tokens.reshape(\u001b[94m1\u001b[0m,\u001b[94m1\u001b[0m,-\u001b[94m1\u001b[0m).to(embeddings.device),    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m block \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.transformer.h:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m110 \u001b[2m│   │   │   \u001b[0mx = block(x)                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.transformer.ln_f(x)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.lm_head(x)  \u001b[2m# (b, t, vocab_size)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m138\u001b[0m in \u001b[92mforward\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x: torch.Tensor) -> torch.Tensor:                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m138 \u001b[2m│   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.attn(\u001b[96mself\u001b[0m.rms_1(x))                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.mlp(\u001b[96mself\u001b[0m.rms_2(x))                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m198\u001b[0m in \u001b[92mforward\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdevice=x.device,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m198 \u001b[2m│   │   \u001b[0mq = apply_rope(q, \u001b[96mself\u001b[0m.rope_cache)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0mk = apply_rope(k, \u001b[96mself\u001b[0m.rope_cache)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh,\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m292\u001b[0m in \u001b[92mapply_rope\u001b[0m                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m289 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# cast because the reference does\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   \u001b[0mxshaped = x.float().reshape(*x.shape[:-\u001b[94m1\u001b[0m], -\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   \u001b[0mrope_cache = rope_cache.view(\u001b[94m1\u001b[0m, xshaped.size(\u001b[94m1\u001b[0m), \u001b[94m1\u001b[0m, xshaped.size(\u001b[94m3\u001b[0m), \u001b[94m2\u001b[0m)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m292 \u001b[2m│   \u001b[0mx_out2 = torch.stack(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m293 \u001b[0m\u001b[2m│   │   \u001b[0m[xshaped[..., \u001b[94m0\u001b[0m] * rope_cache[..., \u001b[94m0\u001b[0m] - xshaped[..., \u001b[94m1\u001b[0m] * rope_cache[..., \u001b[94m1\u001b[0m],      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │    \u001b[0mxshaped[..., \u001b[94m1\u001b[0m] * rope_cache[..., \u001b[94m0\u001b[0m] + xshaped[..., \u001b[94m0\u001b[0m] * rope_cache[..., \u001b[94m1\u001b[0m],      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   │   \u001b[0m], -\u001b[94m1\u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for param in LLamaModel.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "LLamaModel.eval()\n",
    "for epoch in range(1):\n",
    "\n",
    "    indices = list(range(len(squad_train)))\n",
    "    random.shuffle(indices)\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    while(len(indices) >= batch_size):\n",
    "        batch_indices = indices[:batch_size]\n",
    "        indices = indices[batch_size:]\n",
    "        batch_loss = 0\n",
    "\n",
    "        IST_generator.train()\n",
    "        optimizer.zero_grad()\n",
    "        for i in range(batch_size):\n",
    "            input, target = get_single_example(squad_train, index=batch_indices[i])\n",
    "            llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "            loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "            del llama_output\n",
    "\n",
    "        batch_loss /= batch_size\n",
    "\n",
    "        optimizer.step()\n",
    "        train_losses.append(batch_loss)\n",
    "        epoch_train_loss += batch_loss\n",
    "\n",
    "        # validation:\n",
    "        IST_generator.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_loss = 0\n",
    "            for i in range(batch_size):\n",
    "                input, target = get_single_example(squad_test)\n",
    "                llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "                loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "                del llama_output\n",
    "                batch_loss += loss.item()\n",
    "            batch_loss /= batch_size\n",
    "\n",
    "            test_losses.append(batch_loss)\n",
    "            \n",
    "        print(f'epoch {epoch}, train loss = {train_losses[-1]}, validation loss={test_losses[-1]}')\n",
    "        wandb.log({'batch train loss':train_losses[-1], 'batch validation loss':test_losses[-1]})\n",
    "\n",
    "    # perform validation on entire validation set\n",
    "    with torch.no_grad():\n",
    "        batch_loss = 0\n",
    "        for i in range(1000):\n",
    "            input, target = get_single_example(squad_test, i)\n",
    "            llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "            loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "            del llama_output\n",
    "            batch_loss += loss.item()\n",
    "        batch_loss /= 1000\n",
    "\n",
    "    # wandb.log({'epoch train loss':epoch_train_loss, 'epoch validation loss':batch_loss})\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses[:252], label='normal')\n",
    "plt.plot(train_losses[252:], label='instruction and input inverted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IST(string):\n",
    "    tokens = tokenizer.encode(string).unsqueeze(0).type(torch.LongTensor).to(fabric.device)\n",
    "    x = LLamaModel(tokens)[1]\n",
    "    x = IST_generator(x)\n",
    "    return x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, IST=None, max_new_tokens=200):\n",
    "  \n",
    "    generated = ''\n",
    "    tokenized_input = tokenizer.encode(prompt).to(fabric.device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            last_logits = model(tokenized_input.unsqueeze(0), IST.type(torch.bfloat16))[0][:,-1,:]\n",
    "            new_token = torch.argmax(last_logits, dim=1)\n",
    "            if(new_token == 2 and _ > 0): #eos\n",
    "                break\n",
    "            generated += tokenizer.decode(new_token)\n",
    "            tokenized_input = torch.cat([tokenized_input, new_token])\n",
    "\n",
    "    return tokenized_input, tokenizer.decode(tokenized_input)[len(prompt)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate(LLamaModel, tokenizer, prompt=\"Country: Egypt\", IST=get_IST(\"Write a story about a young boy visiting a foreign country.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate(LLamaModel, tokenizer, prompt=\"Create a news headline for a story about a celebrity who just released a book.\", IST=get_IST(\"Tom Hanks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases New Book\"\\n\"Celebrity Author Releases'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 out = generate(LLamaModel, tokenizer, squad[<span style=\"color: #808000; text-decoration-color: #808000\">'train'</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'question'</span>], IST=get_IST(squad[<span style=\"color: #808000; text-decoration-color: #808000\">'</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'squad'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 out = generate(LLamaModel, tokenizer, squad[\u001b[33m'\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m'\u001b[0m][\u001b[94m0\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mquestion\u001b[0m\u001b[33m'\u001b[0m], IST=get_IST(squad[\u001b[33m'\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'squad'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = generate(LLamaModel, tokenizer, squad['train'][0]['question'], IST=get_IST(squad['train'][0]['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/andrew/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': ' ',\n",
       " 'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56bed07e3aeaaa14008c94a9',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': \"Beyoncé has worked with Pepsi since 2002, and in 2004 appeared in a Gladiator-themed commercial with Britney Spears, Pink, and Enrique Iglesias. In 2012, Beyoncé signed a $50 million deal to endorse Pepsi. The Center for Science in the Public Interest (CSPINET) wrote Beyoncé an open letter asking her to reconsider the deal because of the unhealthiness of the product and to donate the proceeds to a medical organisation. Nevertheless, NetBase found that Beyoncé's campaign was the most talked about endorsement in April 2013, with a 70 per cent positive audience response to the commercial and print ads.\",\n",
       " 'question': 'Which soda company has Beyonce partnered with since 2002?',\n",
       " 'answers': {'text': ['Pepsi'], 'answer_start': [24]}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IST_generator.load_state_dict(torch.load(\"new_instructtunedweights_4.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_test[10]['input'].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty: 855.9619140625\n",
      " Nonempty: 761.69921875\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    empty_loss = 0\n",
    "    nonempty_loss = 0\n",
    "\n",
    "    for i in range(1000):\n",
    "        input, target = get_single_example(squad_test, i)\n",
    "        llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "        loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "\n",
    "        if(squad_test[i]['input'].shape[0] == 1):\n",
    "            empty_loss += loss.item()\n",
    "        else:\n",
    "            nonempty_loss += loss.item()\n",
    "\n",
    "print(f'Empty: {empty_loss}\\nNonempty: {nonempty_loss}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46584"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(squad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if(squad_train[i]['input'][0] == 259 and squad_train[i]['input'].shape[0] == 1):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty: 816.0869140625\n",
      "Nonempty: 976.6796875\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    empty_loss = 0\n",
    "    nonempty_loss = 0\n",
    "\n",
    "    for i in range(1000):\n",
    "        input, target = get_single_example(squad_train, i)\n",
    "        llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "        loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "\n",
    "        if(squad_train[i]['input'][0] == 259 and squad_train[i]['input'].shape[0] == 1):\n",
    "            empty_loss += loss.item()\n",
    "        else:\n",
    "            nonempty_loss += loss.item()\n",
    "\n",
    "print(f'Empty: {empty_loss}\\nNonempty: {nonempty_loss}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wojciech Żywny'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lit_llama import model\n",
    "import random\n",
    "from lit_llama import LLaMA, Tokenizer\n",
    "from lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fabric = L.Fabric(devices=1)\n",
    "tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/andrew/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56ce7bf4aab44d1400b887f5', 'title': 'IPod', 'context': 'The name iPod was proposed by Vinnie Chieco, a freelance copywriter, who (with others) was called by Apple to figure out how to introduce the new player to the public. After Chieco saw a prototype, he thought of the movie 2001: A Space Odyssey and the phrase \"Open the pod bay door, Hal!\", which refers to the white EVA Pods of the Discovery One spaceship. Chieco saw an analogy to the relationship between the spaceship and the smaller independent pods in the relationship between a personal computer and the music player. Apple researched the trademark and found that it was already in use. Joseph N. Grasso of New Jersey had originally listed an \"iPod\" trademark with the U.S. Patent and Trademark Office (USPTO) in July 2000 for Internet kiosks. The first iPod kiosks had been demonstrated to the public in New Jersey in March 1998, and commercial use began in January 2000, but had apparently been discontinued by 2001. The trademark was registered by the USPTO in November 2003, and Grasso assigned it to Apple Computer, Inc. in 2005.', 'question': 'Who held the original trademark for the iPod name?', 'answers': {'text': ['Joseph N. Grasso'], 'answer_start': [593]}}\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('datasets/alpaca_data_cleaned.json') as f:\n",
    "    alpaca_json = json.load(f)\n",
    "\n",
    "# Create tokenized j\n",
    "squad_train = []\n",
    "squad_test = []\n",
    "\n",
    "for item in squad['train']:\n",
    "    squad_train.append(\n",
    "        {\n",
    "            'instruction': tokenizer.encode(item['context'], bos=True, eos=False, device=fabric.device),\n",
    "            'input': tokenizer.encode(item['question'], bos=False, eos=False, device=fabric.device),\n",
    "            'output':tokenizer.encode(item['answers']['text'][0], bos=False, eos=True, device=fabric.device)\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "for item in squad['test']:\n",
    "    squad_test.append(\n",
    "        {\n",
    "            'instruction': tokenizer.encode(item['context'], bos=True, eos=False, device=fabric.device),\n",
    "            'input': tokenizer.encode(item['question'], bos=False, eos=False, device=fabric.device),\n",
    "            'output':tokenizer.encode(item['answers']['text'][0], bos=False, eos=True, device=fabric.device)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_example(dataset, index=None):\n",
    "    if(index is None):\n",
    "        index = random.sample(range(len(dataset)), k=1)[0]\n",
    "    # IST\n",
    "    IST = IST_generator(LLamaModel(dataset[index]['instruction'].unsqueeze(0).to(fabric.device))[1])[:,-1,:]\n",
    "\n",
    "    # Question\n",
    "    question = LLamaModel.transformer.wte(dataset[index]['input'].unsqueeze(0).to(fabric.device)).squeeze()\n",
    "\n",
    "    # Answer fragment\n",
    "    answer_len = dataset[index]['output'].size(0)\n",
    "    trunc_len = random.randint(0,answer_len-1)\n",
    "    #print(answer_len)\n",
    "    #print(trunc_len)\n",
    "\n",
    "    truncated_answer = dataset[index]['output'][:trunc_len]\n",
    "    truncated_answer = LLamaModel.transformer.wte(truncated_answer)\n",
    "    \n",
    "    target_tokens = torch.cat([dataset[index]['input'], dataset[index]['output'][:trunc_len+1]])\n",
    "    #print(tokenizer.decode(target_tokens))\n",
    "\n",
    "    if(question.dim() == 1):\n",
    "        question = question.unsqueeze(0)\n",
    "\n",
    "    if(truncated_answer.dim() == 1):\n",
    "        truncated_answer = truncated_answer.unsqueeze(0)\n",
    "\n",
    "    llama_input = torch.cat([IST,question,truncated_answer])\n",
    "    return llama_input.unsqueeze(0), target_tokens.type(torch.LongTensor).unsqueeze(0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\")\n",
    "tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "\n",
    "\n",
    "def load_LLaMA(checkpoint_path):\n",
    "    with lazy_load(checkpoint_path) as checkpoint:\n",
    "        name = llama_model_lookup(checkpoint)\n",
    "\n",
    "        with EmptyInitOnDevice(\n",
    "                device=fabric.device, dtype=dtype, quantization_mode=None # We won't quantize the weights\n",
    "        ):\n",
    "            model = LLaMA.from_name(name)\n",
    "\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Finished loading the first model\n",
      "Finished loading models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "\n",
    "LLaMA_config = model.LLaMAConfig.from_name('7B')\n",
    "print('Loading models...')\n",
    "# Load the LLaMa model and the IST generator (also a LLaMA model)\n",
    "LLamaModel = load_LLaMA(checkpoint_path).to(fabric.device)\n",
    "#LLamaModel = LLaMA(LLaMA_config).to(fabric.device)\n",
    "print('Finished loading the first model')\n",
    "print('Finished loading models')\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "IST_schemes = ['vanilla', 'last 4', '2nd to last', 'all layers']\n",
    "scheme_losses = {}\n",
    "\n",
    "IST_generator = model.Block(LLaMA_config)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(IST_generator.parameters(), lr=1e-4)\n",
    "IST_generator = IST_generator.to(fabric.device)\n",
    "\n",
    "for param in LLamaModel.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(IST_generator.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size=32\n",
    "trainset_size=4000\n",
    "testset_size=1000\n",
    "\n",
    "config = {\n",
    "    'lr': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'trainset_size': trainset_size,\n",
    "    'testset_size':testset_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrew-zeng\u001b[0m (\u001b[33msmalllanguagemodels\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andrew/Documents/lit-llama/wandb/run-20230630_095659-i9tb5aob</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/smalllanguagemodels/IST%20QA/runs/i9tb5aob' target=\"_blank\">Training LLama on the SQuAD dataset</a></strong> to <a href='https://wandb.ai/smalllanguagemodels/IST%20QA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/smalllanguagemodels/IST%20QA' target=\"_blank\">https://wandb.ai/smalllanguagemodels/IST%20QA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/smalllanguagemodels/IST%20QA/runs/i9tb5aob' target=\"_blank\">https://wandb.ai/smalllanguagemodels/IST%20QA/runs/i9tb5aob</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/smalllanguagemodels/IST%20QA/runs/i9tb5aob?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc4c4f95cc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init wandb\n",
    "wandb.init(\n",
    "    project='IST QA',\n",
    "    config=config,\n",
    "    name=\"Training LLama on the SQuAD dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def filter_string(input_string):\n",
    "    input_string = input_string.lower()\n",
    "    filtered_string = re.sub(r'[^a-zA-Z0-9\\s]', '', input_string)\n",
    "    filtered_string = re.sub(r'\\n', ' ', filtered_string)\n",
    "    filtered_string = re.sub(r' +', ' ', filtered_string)\n",
    "    return filtered_string\n",
    "\n",
    "# Example usage\n",
    "\n",
    "def calculate_F1_score(\n",
    "        model_output: str,\n",
    "        ground_truth_output: str,\n",
    "):\n",
    "    model_output_words = set(filter_string(model_output).split(' '))\n",
    "    ground_truth_words = set(filter_string(ground_truth_output).split(' '))\n",
    "    \n",
    "    shared_words = model_output_words & ground_truth_words\n",
    "    if(len(shared_words) == 0):\n",
    "        return 0\n",
    "    #print(shared_words)\n",
    "\n",
    "    precision = len(shared_words) / len(model_output_words)\n",
    "    recall = len(shared_words) / len(ground_truth_words)\n",
    "\n",
    "    return 2 / (1/recall + 1/precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IST(string):\n",
    "    tokens = tokenizer.encode(string).unsqueeze(0).type(torch.LongTensor).to(fabric.device)\n",
    "    x = LLamaModel(tokens)[1]\n",
    "    x = IST_generator(x)\n",
    "    return x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, IST=None, max_new_tokens=200):\n",
    "  \n",
    "    generated = ''\n",
    "    tokenized_input = tokenizer.encode(prompt).to(fabric.device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            last_logits = model(tokenized_input.unsqueeze(0), IST.type(torch.bfloat16))[0][:,-1,:]\n",
    "            new_token = torch.argmax(last_logits, dim=1)\n",
    "            if(new_token == 2 and _ > 0): #eos\n",
    "                break\n",
    "            generated += tokenizer.decode(new_token)\n",
    "            tokenized_input = torch.cat([tokenized_input, new_token])\n",
    "\n",
    "    return tokenized_input, tokenizer.decode(tokenized_input)[len(prompt)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "total_f1s = 0\n",
    "for index, item in enumerate(list(squad['test'])[:10] ):\n",
    "\n",
    "    context = item['context']\n",
    "    question = item['question']\n",
    "    answer = item['answers']['text'][0]\n",
    "    _, out = generate(LLamaModel, tokenizer, question, IST=get_IST(context),max_new_tokens=len(tokenizer.encode(answer)))\n",
    "    total_f1s += calculate_F1_score(out, answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch': 0, 'batch train loss': 7.98193359375, 'batch validation loss': 7.71435546875, 'F1 score': 0.045}\n",
      "{'batch': 1, 'batch train loss': 7.45166015625, 'batch validation loss': 6.794921875}\n",
      "{'batch': 2, 'batch train loss': 6.45703125, 'batch validation loss': 5.427734375}\n",
      "{'batch': 3, 'batch train loss': 6.18212890625, 'batch validation loss': 4.7080078125}\n",
      "{'batch': 4, 'batch train loss': 5.138671875, 'batch validation loss': 4.3916015625}\n",
      "{'batch': 5, 'batch train loss': 5.0234375, 'batch validation loss': 4.1669921875}\n",
      "{'batch': 6, 'batch train loss': 4.525390625, 'batch validation loss': 4.12353515625}\n",
      "{'batch': 7, 'batch train loss': 4.40283203125, 'batch validation loss': 3.9921875}\n",
      "{'batch': 8, 'batch train loss': 4.251953125, 'batch validation loss': 4.11962890625}\n",
      "{'batch': 9, 'batch train loss': 4.43017578125, 'batch validation loss': 4.1435546875}\n",
      "{'batch': 10, 'batch train loss': 4.2275390625, 'batch validation loss': 4.0556640625, 'F1 score': 0.05333333333333333}\n",
      "{'batch': 11, 'batch train loss': 5.00048828125, 'batch validation loss': 3.99853515625}\n",
      "{'batch': 12, 'batch train loss': 4.162109375, 'batch validation loss': 4.0625}\n",
      "{'batch': 13, 'batch train loss': 4.466796875, 'batch validation loss': 4.0029296875}\n",
      "{'batch': 14, 'batch train loss': 3.99267578125, 'batch validation loss': 4.0498046875}\n",
      "{'batch': 15, 'batch train loss': 4.12939453125, 'batch validation loss': 3.98193359375}\n",
      "{'batch': 16, 'batch train loss': 4.18896484375, 'batch validation loss': 3.8896484375}\n",
      "{'batch': 17, 'batch train loss': 4.03564453125, 'batch validation loss': 3.83544921875}\n",
      "{'batch': 18, 'batch train loss': 4.2119140625, 'batch validation loss': 3.79052734375}\n",
      "{'batch': 19, 'batch train loss': 4.0205078125, 'batch validation loss': 3.75830078125}\n",
      "{'batch': 20, 'batch train loss': 4.33349609375, 'batch validation loss': 3.8779296875, 'F1 score': 0.05333333333333333}\n",
      "{'batch': 21, 'batch train loss': 4.2880859375, 'batch validation loss': 3.80712890625}\n",
      "{'batch': 22, 'batch train loss': 3.813720703125, 'batch validation loss': 3.720703125}\n",
      "{'batch': 23, 'batch train loss': 4.021484375, 'batch validation loss': 3.689453125}\n",
      "{'batch': 24, 'batch train loss': 3.93603515625, 'batch validation loss': 3.65185546875}\n",
      "{'batch': 25, 'batch train loss': 4.236328125, 'batch validation loss': 3.6845703125}\n",
      "{'batch': 26, 'batch train loss': 3.9736328125, 'batch validation loss': 3.68701171875}\n",
      "{'batch': 27, 'batch train loss': 3.79541015625, 'batch validation loss': 3.62451171875}\n",
      "{'batch': 28, 'batch train loss': 3.984375, 'batch validation loss': 3.6259765625}\n",
      "{'batch': 29, 'batch train loss': 3.82177734375, 'batch validation loss': 3.58740234375}\n",
      "{'batch': 30, 'batch train loss': 4.18896484375, 'batch validation loss': 3.4541015625, 'F1 score': 0.05333333333333333}\n",
      "{'batch': 31, 'batch train loss': 3.82275390625, 'batch validation loss': 3.455810546875}\n",
      "{'batch': 32, 'batch train loss': 3.69482421875, 'batch validation loss': 3.414306640625}\n",
      "{'batch': 33, 'batch train loss': 3.95849609375, 'batch validation loss': 3.373291015625}\n",
      "{'batch': 34, 'batch train loss': 3.5400390625, 'batch validation loss': 3.3642578125}\n",
      "{'batch': 35, 'batch train loss': 3.60986328125, 'batch validation loss': 3.33251953125}\n",
      "{'batch': 36, 'batch train loss': 3.614013671875, 'batch validation loss': 3.3017578125}\n",
      "{'batch': 37, 'batch train loss': 3.69140625, 'batch validation loss': 3.245849609375}\n",
      "{'batch': 38, 'batch train loss': 3.45068359375, 'batch validation loss': 3.293701171875}\n",
      "{'batch': 39, 'batch train loss': 3.6416015625, 'batch validation loss': 3.230712890625}\n",
      "{'batch': 40, 'batch train loss': 3.86376953125, 'batch validation loss': 3.2021484375, 'F1 score': 0.05333333333333333}\n",
      "{'batch': 41, 'batch train loss': 3.54052734375, 'batch validation loss': 3.184814453125}\n",
      "{'batch': 42, 'batch train loss': 3.600341796875, 'batch validation loss': 3.16748046875}\n",
      "{'batch': 43, 'batch train loss': 3.482421875, 'batch validation loss': 3.12255859375}\n",
      "{'batch': 44, 'batch train loss': 3.30712890625, 'batch validation loss': 3.122314453125}\n",
      "{'batch': 45, 'batch train loss': 3.458740234375, 'batch validation loss': 3.088623046875}\n",
      "{'batch': 46, 'batch train loss': 3.50634765625, 'batch validation loss': 3.06982421875}\n",
      "{'batch': 47, 'batch train loss': 3.597412109375, 'batch validation loss': 3.02734375}\n",
      "{'batch': 48, 'batch train loss': 3.47802734375, 'batch validation loss': 3.02099609375}\n",
      "{'batch': 49, 'batch train loss': 3.310546875, 'batch validation loss': 3.01123046875}\n",
      "{'batch': 50, 'batch train loss': 2.912109375, 'batch validation loss': 2.991455078125, 'F1 score': 0.05333333333333333}\n",
      "{'batch': 51, 'batch train loss': 3.56982421875, 'batch validation loss': 2.91943359375}\n",
      "{'batch': 52, 'batch train loss': 2.94091796875, 'batch validation loss': 2.98046875}\n",
      "{'batch': 53, 'batch train loss': 3.6572265625, 'batch validation loss': 2.917724609375}\n",
      "{'batch': 54, 'batch train loss': 3.166015625, 'batch validation loss': 2.9384765625}\n",
      "{'batch': 55, 'batch train loss': 3.201171875, 'batch validation loss': 2.953857421875}\n",
      "{'batch': 56, 'batch train loss': 3.126708984375, 'batch validation loss': 2.83447265625}\n",
      "{'batch': 57, 'batch train loss': 2.969482421875, 'batch validation loss': 2.847900390625}\n",
      "{'batch': 58, 'batch train loss': 2.990966796875, 'batch validation loss': 2.76708984375}\n",
      "{'batch': 59, 'batch train loss': 2.65087890625, 'batch validation loss': 2.760009765625}\n",
      "{'batch': 60, 'batch train loss': 3.34228515625, 'batch validation loss': 2.773681640625, 'F1 score': 0.045}\n",
      "{'batch': 61, 'batch train loss': 3.2587890625, 'batch validation loss': 2.63623046875}\n",
      "{'batch': 62, 'batch train loss': 3.19189453125, 'batch validation loss': 2.738037109375}\n",
      "{'batch': 63, 'batch train loss': 2.996826171875, 'batch validation loss': 2.653076171875}\n",
      "{'batch': 64, 'batch train loss': 2.829833984375, 'batch validation loss': 2.65185546875}\n",
      "{'batch': 65, 'batch train loss': 3.188720703125, 'batch validation loss': 2.656982421875}\n",
      "{'batch': 66, 'batch train loss': 2.997314453125, 'batch validation loss': 2.558837890625}\n",
      "{'batch': 67, 'batch train loss': 2.998046875, 'batch validation loss': 2.614013671875}\n",
      "{'batch': 68, 'batch train loss': 3.230712890625, 'batch validation loss': 2.57861328125}\n",
      "{'batch': 69, 'batch train loss': 2.9052734375, 'batch validation loss': 2.56298828125}\n",
      "{'batch': 70, 'batch train loss': 2.85595703125, 'batch validation loss': 2.565673828125, 'F1 score': 0.05333333333333333}\n",
      "{'batch': 71, 'batch train loss': 2.8974609375, 'batch validation loss': 2.608154296875}\n",
      "{'batch': 72, 'batch train loss': 2.993408203125, 'batch validation loss': 2.50439453125}\n",
      "{'batch': 73, 'batch train loss': 2.71728515625, 'batch validation loss': 2.5341796875}\n",
      "{'batch': 74, 'batch train loss': 2.728271484375, 'batch validation loss': 2.49853515625}\n",
      "{'batch': 75, 'batch train loss': 2.954833984375, 'batch validation loss': 2.409912109375}\n",
      "{'batch': 76, 'batch train loss': 2.662109375, 'batch validation loss': 2.450439453125}\n",
      "{'batch': 77, 'batch train loss': 2.642822265625, 'batch validation loss': 2.439697265625}\n",
      "{'batch': 78, 'batch train loss': 2.71337890625, 'batch validation loss': 2.451416015625}\n",
      "{'batch': 79, 'batch train loss': 2.67138671875, 'batch validation loss': 2.432373046875}\n",
      "{'batch': 80, 'batch train loss': 2.79443359375, 'batch validation loss': 2.4833984375, 'F1 score': 0.05333333333333333}\n",
      "{'batch': 81, 'batch train loss': 2.264404296875, 'batch validation loss': 2.396484375}\n",
      "{'batch': 82, 'batch train loss': 2.545166015625, 'batch validation loss': 2.404296875}\n",
      "{'batch': 83, 'batch train loss': 3.160888671875, 'batch validation loss': 2.39794921875}\n",
      "{'batch': 84, 'batch train loss': 2.9208984375, 'batch validation loss': 2.310302734375}\n",
      "{'batch': 85, 'batch train loss': 2.501220703125, 'batch validation loss': 2.35546875}\n",
      "{'batch': 86, 'batch train loss': 2.557373046875, 'batch validation loss': 2.355712890625}\n",
      "{'batch': 87, 'batch train loss': 2.85498046875, 'batch validation loss': 2.361572265625}\n",
      "{'batch': 88, 'batch train loss': 2.62548828125, 'batch validation loss': 2.300048828125}\n",
      "{'batch': 89, 'batch train loss': 2.5126953125, 'batch validation loss': 2.28759765625}\n",
      "{'batch': 90, 'batch train loss': 2.563720703125, 'batch validation loss': 2.339599609375, 'F1 score': 0.12571428571428572}\n",
      "{'batch': 91, 'batch train loss': 2.59619140625, 'batch validation loss': 2.2484130859375}\n",
      "{'batch': 92, 'batch train loss': 2.5712890625, 'batch validation loss': 2.3134765625}\n",
      "{'batch': 93, 'batch train loss': 2.7978515625, 'batch validation loss': 2.267822265625}\n",
      "{'batch': 94, 'batch train loss': 2.658447265625, 'batch validation loss': 2.285400390625}\n",
      "{'batch': 95, 'batch train loss': 2.603759765625, 'batch validation loss': 2.2467041015625}\n",
      "{'batch': 96, 'batch train loss': 2.53857421875, 'batch validation loss': 2.291015625}\n",
      "{'batch': 97, 'batch train loss': 2.651611328125, 'batch validation loss': 2.2076416015625}\n",
      "{'batch': 98, 'batch train loss': 2.91259765625, 'batch validation loss': 2.236083984375}\n",
      "{'batch': 99, 'batch train loss': 2.6494140625, 'batch validation loss': 2.282958984375}\n",
      "{'batch': 100, 'batch train loss': 2.75, 'batch validation loss': 2.2353515625, 'F1 score': 0.12571428571428572}\n",
      "{'batch': 101, 'batch train loss': 2.65966796875, 'batch validation loss': 2.188720703125}\n",
      "{'batch': 102, 'batch train loss': 2.443359375, 'batch validation loss': 2.265380859375}\n",
      "{'batch': 103, 'batch train loss': 2.740966796875, 'batch validation loss': 2.24951171875}\n",
      "{'batch': 104, 'batch train loss': 2.635498046875, 'batch validation loss': 2.197509765625}\n",
      "{'batch': 105, 'batch train loss': 2.513427734375, 'batch validation loss': 2.1845703125}\n",
      "{'batch': 106, 'batch train loss': 2.551025390625, 'batch validation loss': 2.1934814453125}\n",
      "{'batch': 107, 'batch train loss': 2.7275390625, 'batch validation loss': 2.1724853515625}\n",
      "{'batch': 108, 'batch train loss': 2.5146484375, 'batch validation loss': 2.243896484375}\n",
      "{'batch': 109, 'batch train loss': 2.611083984375, 'batch validation loss': 2.229736328125}\n",
      "{'batch': 110, 'batch train loss': 2.1695556640625, 'batch validation loss': 2.156005859375, 'F1 score': 0.12571428571428572}\n",
      "{'batch': 111, 'batch train loss': 2.333740234375, 'batch validation loss': 2.212158203125}\n",
      "{'batch': 112, 'batch train loss': 2.48828125, 'batch validation loss': 2.18603515625}\n",
      "{'batch': 113, 'batch train loss': 2.808837890625, 'batch validation loss': 2.2152099609375}\n",
      "{'batch': 114, 'batch train loss': 2.616455078125, 'batch validation loss': 2.1729736328125}\n",
      "{'batch': 115, 'batch train loss': 2.44189453125, 'batch validation loss': 2.17822265625}\n",
      "{'batch': 116, 'batch train loss': 2.268798828125, 'batch validation loss': 2.224609375}\n",
      "{'batch': 117, 'batch train loss': 2.3486328125, 'batch validation loss': 2.19287109375}\n",
      "{'batch': 118, 'batch train loss': 2.72412109375, 'batch validation loss': 2.1898193359375}\n",
      "{'batch': 119, 'batch train loss': 2.370849609375, 'batch validation loss': 2.1292724609375}\n",
      "{'batch': 120, 'batch train loss': 2.718505859375, 'batch validation loss': 2.179931640625, 'F1 score': 0.12571428571428572}\n",
      "{'batch': 121, 'batch train loss': 2.56396484375, 'batch validation loss': 2.224609375}\n",
      "{'batch': 122, 'batch train loss': 2.69775390625, 'batch validation loss': 2.1748046875}\n",
      "{'batch': 123, 'batch train loss': 2.340087890625, 'batch validation loss': 2.144775390625}\n",
      "{'batch': 124, 'batch train loss': 2.56396484375, 'batch validation loss': 2.21337890625}\n",
      "{'batch': 125, 'batch train loss': 2.153076171875, 'batch validation loss': 2.1982421875, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 126, 'batch train loss': 2.343017578125, 'batch validation loss': 2.1065673828125}\n",
      "{'batch': 127, 'batch train loss': 2.162109375, 'batch validation loss': 2.117919921875}\n",
      "{'batch': 128, 'batch train loss': 2.388916015625, 'batch validation loss': 2.1436767578125}\n",
      "{'batch': 129, 'batch train loss': 2.35009765625, 'batch validation loss': 2.181396484375}\n",
      "{'batch': 130, 'batch train loss': 2.41943359375, 'batch validation loss': 2.158935546875}\n",
      "{'batch': 131, 'batch train loss': 2.569091796875, 'batch validation loss': 2.1751708984375}\n",
      "{'batch': 132, 'batch train loss': 2.490234375, 'batch validation loss': 2.1278076171875}\n",
      "{'batch': 133, 'batch train loss': 2.5009765625, 'batch validation loss': 2.1563720703125}\n",
      "{'batch': 134, 'batch train loss': 2.283935546875, 'batch validation loss': 2.1058349609375}\n",
      "{'batch': 135, 'batch train loss': 2.3043212890625, 'batch validation loss': 2.1077880859375, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 136, 'batch train loss': 2.390380859375, 'batch validation loss': 2.076904296875}\n",
      "{'batch': 137, 'batch train loss': 2.470458984375, 'batch validation loss': 2.149169921875}\n",
      "{'batch': 138, 'batch train loss': 2.535400390625, 'batch validation loss': 2.1041259765625}\n",
      "{'batch': 139, 'batch train loss': 2.48583984375, 'batch validation loss': 2.0604248046875}\n",
      "{'batch': 140, 'batch train loss': 2.410888671875, 'batch validation loss': 2.154296875}\n",
      "{'batch': 141, 'batch train loss': 2.2464599609375, 'batch validation loss': 2.12890625}\n",
      "{'batch': 142, 'batch train loss': 2.2686767578125, 'batch validation loss': 2.073486328125}\n",
      "{'batch': 143, 'batch train loss': 2.323974609375, 'batch validation loss': 2.107177734375}\n",
      "{'batch': 144, 'batch train loss': 2.220703125, 'batch validation loss': 2.1727294921875}\n",
      "{'batch': 145, 'batch train loss': 2.5947265625, 'batch validation loss': 2.0999755859375, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 146, 'batch train loss': 2.2144775390625, 'batch validation loss': 2.0732421875}\n",
      "{'batch': 147, 'batch train loss': 2.1240234375, 'batch validation loss': 2.123779296875}\n",
      "{'batch': 148, 'batch train loss': 2.367919921875, 'batch validation loss': 2.08349609375}\n",
      "{'batch': 149, 'batch train loss': 2.4169921875, 'batch validation loss': 2.068115234375}\n",
      "{'batch': 150, 'batch train loss': 2.357421875, 'batch validation loss': 2.1065673828125}\n",
      "{'batch': 151, 'batch train loss': 2.222412109375, 'batch validation loss': 2.154052734375}\n",
      "{'batch': 152, 'batch train loss': 2.396484375, 'batch validation loss': 2.0675048828125}\n",
      "{'batch': 153, 'batch train loss': 2.1624755859375, 'batch validation loss': 2.0704345703125}\n",
      "{'batch': 154, 'batch train loss': 2.3629150390625, 'batch validation loss': 2.0897216796875}\n",
      "{'batch': 155, 'batch train loss': 2.1759033203125, 'batch validation loss': 2.0810546875, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 156, 'batch train loss': 2.5009765625, 'batch validation loss': 2.1221923828125}\n",
      "{'batch': 157, 'batch train loss': 2.303466796875, 'batch validation loss': 2.079833984375}\n",
      "{'batch': 158, 'batch train loss': 2.533203125, 'batch validation loss': 2.1058349609375}\n",
      "{'batch': 159, 'batch train loss': 2.398193359375, 'batch validation loss': 2.0830078125}\n",
      "{'batch': 160, 'batch train loss': 2.439453125, 'batch validation loss': 2.0411376953125}\n",
      "{'batch': 161, 'batch train loss': 2.372314453125, 'batch validation loss': 2.119140625}\n",
      "{'batch': 162, 'batch train loss': 2.29443359375, 'batch validation loss': 2.089599609375}\n",
      "{'batch': 163, 'batch train loss': 2.25927734375, 'batch validation loss': 2.0931396484375}\n",
      "{'batch': 164, 'batch train loss': 2.37353515625, 'batch validation loss': 2.042724609375}\n",
      "{'batch': 165, 'batch train loss': 2.265869140625, 'batch validation loss': 2.097412109375, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 166, 'batch train loss': 2.2235107421875, 'batch validation loss': 2.02978515625}\n",
      "{'batch': 167, 'batch train loss': 2.399169921875, 'batch validation loss': 2.078125}\n",
      "{'batch': 168, 'batch train loss': 2.203369140625, 'batch validation loss': 2.0791015625}\n",
      "{'batch': 169, 'batch train loss': 2.390869140625, 'batch validation loss': 2.067138671875}\n",
      "{'batch': 170, 'batch train loss': 2.696044921875, 'batch validation loss': 2.0546875}\n",
      "{'batch': 171, 'batch train loss': 2.3233642578125, 'batch validation loss': 2.0355224609375}\n",
      "{'batch': 172, 'batch train loss': 2.2418212890625, 'batch validation loss': 2.0106201171875}\n",
      "{'batch': 173, 'batch train loss': 2.320556640625, 'batch validation loss': 2.080810546875}\n",
      "{'batch': 174, 'batch train loss': 2.303955078125, 'batch validation loss': 2.031982421875}\n",
      "{'batch': 175, 'batch train loss': 2.115478515625, 'batch validation loss': 2.0689697265625, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 176, 'batch train loss': 2.162109375, 'batch validation loss': 2.0360107421875}\n",
      "{'batch': 177, 'batch train loss': 2.24169921875, 'batch validation loss': 2.0841064453125}\n",
      "{'batch': 178, 'batch train loss': 2.1668701171875, 'batch validation loss': 2.098388671875}\n",
      "{'batch': 179, 'batch train loss': 2.119873046875, 'batch validation loss': 2.07958984375}\n",
      "{'batch': 180, 'batch train loss': 2.302001953125, 'batch validation loss': 2.0655517578125}\n",
      "{'batch': 181, 'batch train loss': 2.20703125, 'batch validation loss': 1.9989013671875}\n",
      "{'batch': 182, 'batch train loss': 2.03955078125, 'batch validation loss': 2.047607421875}\n",
      "{'batch': 183, 'batch train loss': 2.24560546875, 'batch validation loss': 2.06005859375}\n",
      "{'batch': 184, 'batch train loss': 2.287353515625, 'batch validation loss': 2.0728759765625}\n",
      "{'batch': 185, 'batch train loss': 2.1845703125, 'batch validation loss': 2.0870361328125, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 186, 'batch train loss': 2.1395263671875, 'batch validation loss': 2.1036376953125}\n",
      "{'batch': 187, 'batch train loss': 2.238037109375, 'batch validation loss': 2.052001953125}\n",
      "{'batch': 188, 'batch train loss': 2.2615966796875, 'batch validation loss': 2.0186767578125}\n",
      "{'batch': 189, 'batch train loss': 2.3485107421875, 'batch validation loss': 2.0338134765625}\n",
      "{'batch': 190, 'batch train loss': 2.1087646484375, 'batch validation loss': 2.0731201171875}\n",
      "{'batch': 191, 'batch train loss': 1.9903564453125, 'batch validation loss': 1.9937744140625}\n",
      "{'batch': 192, 'batch train loss': 2.2059326171875, 'batch validation loss': 2.046875}\n",
      "{'batch': 193, 'batch train loss': 2.490478515625, 'batch validation loss': 2.0350341796875}\n",
      "{'batch': 194, 'batch train loss': 2.22509765625, 'batch validation loss': 2.0435791015625}\n",
      "{'batch': 195, 'batch train loss': 2.1787109375, 'batch validation loss': 1.9989013671875, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 196, 'batch train loss': 2.2269287109375, 'batch validation loss': 2.012939453125}\n",
      "{'batch': 197, 'batch train loss': 2.2071533203125, 'batch validation loss': 2.051025390625}\n",
      "{'batch': 198, 'batch train loss': 2.318603515625, 'batch validation loss': 2.0279541015625}\n",
      "{'batch': 199, 'batch train loss': 2.2625732421875, 'batch validation loss': 2.05029296875}\n",
      "{'batch': 200, 'batch train loss': 2.3115234375, 'batch validation loss': 2.013916015625}\n",
      "{'batch': 201, 'batch train loss': 2.2392578125, 'batch validation loss': 2.003173828125}\n",
      "{'batch': 202, 'batch train loss': 2.385986328125, 'batch validation loss': 2.048828125}\n",
      "{'batch': 203, 'batch train loss': 2.119384765625, 'batch validation loss': 1.99951171875}\n",
      "{'batch': 204, 'batch train loss': 2.0723876953125, 'batch validation loss': 1.9871826171875}\n",
      "{'batch': 205, 'batch train loss': 2.2376708984375, 'batch validation loss': 1.979736328125, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 206, 'batch train loss': 2.2396240234375, 'batch validation loss': 2.0404052734375}\n",
      "{'batch': 207, 'batch train loss': 2.251708984375, 'batch validation loss': 2.0418701171875}\n",
      "{'batch': 208, 'batch train loss': 2.276123046875, 'batch validation loss': 2.0106201171875}\n",
      "{'batch': 209, 'batch train loss': 2.446533203125, 'batch validation loss': 1.973388671875}\n",
      "{'batch': 210, 'batch train loss': 2.041748046875, 'batch validation loss': 2.034912109375}\n",
      "{'batch': 211, 'batch train loss': 2.02783203125, 'batch validation loss': 2.00341796875}\n",
      "{'batch': 212, 'batch train loss': 2.29443359375, 'batch validation loss': 2.0458984375}\n",
      "{'batch': 213, 'batch train loss': 2.1842041015625, 'batch validation loss': 2.0068359375}\n",
      "{'batch': 214, 'batch train loss': 2.086181640625, 'batch validation loss': 1.997314453125}\n",
      "{'batch': 215, 'batch train loss': 2.072021484375, 'batch validation loss': 1.997802734375, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 216, 'batch train loss': 2.2919921875, 'batch validation loss': 2.0577392578125}\n",
      "{'batch': 217, 'batch train loss': 2.357177734375, 'batch validation loss': 2.053466796875}\n",
      "{'batch': 218, 'batch train loss': 2.3544921875, 'batch validation loss': 2.0325927734375}\n",
      "{'batch': 219, 'batch train loss': 2.0789794921875, 'batch validation loss': 1.9703369140625}\n",
      "{'batch': 220, 'batch train loss': 2.25048828125, 'batch validation loss': 2.0537109375}\n",
      "{'batch': 221, 'batch train loss': 2.169921875, 'batch validation loss': 2.0181884765625}\n",
      "{'batch': 222, 'batch train loss': 1.930419921875, 'batch validation loss': 2.00439453125}\n",
      "{'batch': 223, 'batch train loss': 2.222900390625, 'batch validation loss': 2.0667724609375}\n",
      "{'batch': 224, 'batch train loss': 2.0640869140625, 'batch validation loss': 2.0654296875}\n",
      "{'batch': 225, 'batch train loss': 2.54931640625, 'batch validation loss': 1.9744873046875, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 226, 'batch train loss': 2.00927734375, 'batch validation loss': 1.9803466796875}\n",
      "{'batch': 227, 'batch train loss': 2.1126708984375, 'batch validation loss': 1.9683837890625}\n",
      "{'batch': 228, 'batch train loss': 2.2847900390625, 'batch validation loss': 1.995361328125}\n",
      "{'batch': 229, 'batch train loss': 2.31591796875, 'batch validation loss': 1.9801025390625}\n",
      "{'batch': 230, 'batch train loss': 2.2030029296875, 'batch validation loss': 2.0654296875}\n",
      "{'batch': 231, 'batch train loss': 2.3818359375, 'batch validation loss': 2.066650390625}\n",
      "{'batch': 232, 'batch train loss': 1.99169921875, 'batch validation loss': 2.08056640625}\n",
      "{'batch': 233, 'batch train loss': 2.237548828125, 'batch validation loss': 2.00537109375}\n",
      "{'batch': 234, 'batch train loss': 2.3494873046875, 'batch validation loss': 2.003662109375}\n",
      "{'batch': 235, 'batch train loss': 2.2353515625, 'batch validation loss': 2.0108642578125, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 236, 'batch train loss': 2.468017578125, 'batch validation loss': 2.00732421875}\n",
      "{'batch': 237, 'batch train loss': 2.21435546875, 'batch validation loss': 1.9970703125}\n",
      "{'batch': 238, 'batch train loss': 2.0191650390625, 'batch validation loss': 2.0238037109375}\n",
      "{'batch': 239, 'batch train loss': 2.36962890625, 'batch validation loss': 2.00244140625}\n",
      "{'batch': 240, 'batch train loss': 2.312744140625, 'batch validation loss': 2.0128173828125}\n",
      "{'batch': 241, 'batch train loss': 2.1171875, 'batch validation loss': 2.0484619140625}\n",
      "{'batch': 242, 'batch train loss': 2.1138916015625, 'batch validation loss': 1.9971923828125}\n",
      "{'batch': 243, 'batch train loss': 2.152099609375, 'batch validation loss': 1.9671630859375}\n",
      "{'batch': 244, 'batch train loss': 2.2158203125, 'batch validation loss': 1.9609375}\n",
      "{'batch': 245, 'batch train loss': 2.396728515625, 'batch validation loss': 2.0093994140625, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 246, 'batch train loss': 2.209716796875, 'batch validation loss': 1.986572265625}\n",
      "{'batch': 247, 'batch train loss': 2.0203857421875, 'batch validation loss': 2.01416015625}\n",
      "{'batch': 248, 'batch train loss': 2.387939453125, 'batch validation loss': 2.0223388671875}\n",
      "{'batch': 249, 'batch train loss': 2.19677734375, 'batch validation loss': 2.014892578125}\n",
      "{'batch': 250, 'batch train loss': 2.2568359375, 'batch validation loss': 1.98193359375, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 251, 'batch train loss': 2.22802734375, 'batch validation loss': 1.970947265625}\n",
      "{'batch': 252, 'batch train loss': 2.0938720703125, 'batch validation loss': 2.0556640625}\n",
      "{'batch': 253, 'batch train loss': 2.2532958984375, 'batch validation loss': 1.9833984375}\n",
      "{'batch': 254, 'batch train loss': 2.2197265625, 'batch validation loss': 1.99462890625}\n",
      "{'batch': 255, 'batch train loss': 2.2587890625, 'batch validation loss': 1.9879150390625}\n",
      "{'batch': 256, 'batch train loss': 2.0936279296875, 'batch validation loss': 1.997802734375}\n",
      "{'batch': 257, 'batch train loss': 2.232177734375, 'batch validation loss': 1.9835205078125}\n",
      "{'batch': 258, 'batch train loss': 2.071044921875, 'batch validation loss': 2.00927734375}\n",
      "{'batch': 259, 'batch train loss': 2.21142578125, 'batch validation loss': 1.9896240234375}\n",
      "{'batch': 260, 'batch train loss': 2.1400146484375, 'batch validation loss': 1.9852294921875, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 261, 'batch train loss': 2.024658203125, 'batch validation loss': 1.9658203125}\n",
      "{'batch': 262, 'batch train loss': 2.1639404296875, 'batch validation loss': 2.0076904296875}\n",
      "{'batch': 263, 'batch train loss': 2.02783203125, 'batch validation loss': 1.9703369140625}\n",
      "{'batch': 264, 'batch train loss': 2.0238037109375, 'batch validation loss': 1.99755859375}\n",
      "{'batch': 265, 'batch train loss': 2.236572265625, 'batch validation loss': 2.023193359375}\n",
      "{'batch': 266, 'batch train loss': 2.3975830078125, 'batch validation loss': 1.9892578125}\n",
      "{'batch': 267, 'batch train loss': 2.1064453125, 'batch validation loss': 1.927734375}\n",
      "{'batch': 268, 'batch train loss': 2.10009765625, 'batch validation loss': 2.0152587890625}\n",
      "{'batch': 269, 'batch train loss': 2.334228515625, 'batch validation loss': 1.9862060546875}\n",
      "{'batch': 270, 'batch train loss': 2.19970703125, 'batch validation loss': 1.9505615234375, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 271, 'batch train loss': 2.0556640625, 'batch validation loss': 1.937255859375}\n",
      "{'batch': 272, 'batch train loss': 2.1094970703125, 'batch validation loss': 1.9656982421875}\n",
      "{'batch': 273, 'batch train loss': 2.072998046875, 'batch validation loss': 1.972412109375}\n",
      "{'batch': 274, 'batch train loss': 2.1253662109375, 'batch validation loss': 1.9788818359375}\n",
      "{'batch': 275, 'batch train loss': 1.9884033203125, 'batch validation loss': 1.96484375}\n",
      "{'batch': 276, 'batch train loss': 2.3272705078125, 'batch validation loss': 1.9930419921875}\n",
      "{'batch': 277, 'batch train loss': 1.9010009765625, 'batch validation loss': 1.9671630859375}\n",
      "{'batch': 278, 'batch train loss': 2.161865234375, 'batch validation loss': 1.96923828125}\n",
      "{'batch': 279, 'batch train loss': 2.3336181640625, 'batch validation loss': 1.99609375}\n",
      "{'batch': 280, 'batch train loss': 2.0545654296875, 'batch validation loss': 1.988525390625, 'F1 score': 0.13999999999999999}\n",
      "{'batch': 281, 'batch train loss': 2.0928955078125, 'batch validation loss': 1.999755859375}\n",
      "{'batch': 282, 'batch train loss': 2.189453125, 'batch validation loss': 1.994384765625}\n",
      "{'batch': 283, 'batch train loss': 2.09765625, 'batch validation loss': 1.996337890625}\n",
      "{'batch': 284, 'batch train loss': 1.9608154296875, 'batch validation loss': 1.974609375}\n",
      "{'batch': 285, 'batch train loss': 1.942138671875, 'batch validation loss': 1.9837646484375}\n",
      "{'batch': 286, 'batch train loss': 2.2806396484375, 'batch validation loss': 2.018798828125}\n",
      "{'batch': 287, 'batch train loss': 2.1138916015625, 'batch validation loss': 1.9256591796875}\n",
      "{'batch': 288, 'batch train loss': 2.317626953125, 'batch validation loss': 1.98974609375}\n",
      "{'batch': 289, 'batch train loss': 2.229248046875, 'batch validation loss': 1.9403076171875}\n",
      "{'batch': 290, 'batch train loss': 2.009765625, 'batch validation loss': 2.0140380859375, 'F1 score': 0.13999999999999999}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">49</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch_loss = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(batch_size):                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>49 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, target = get_single_example(squad_test, index=i)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>llama_output = LLamaModel.forward_embeddings(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>.type(torch.bfloat16))    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze(    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">del</span> llama_output                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_single_example</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span>(index <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>index = random.sample(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(dataset)), k=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># IST</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>IST = IST_generator(LLamaModel(dataset[index][<span style=\"color: #808000; text-decoration-color: #808000\">'instruction'</span>].unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).to(fabric.    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Question</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>question = LLamaModel.transformer.wte(dataset[index][<span style=\"color: #808000; text-decoration-color: #808000\">'input'</span>].unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).to(fabric    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">88</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 85 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = torch.cat((internal_state_tokens.reshape(batch_size,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).to(idx.device)   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 86 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 87 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> block <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer.h:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 88 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = block(x)                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer.ln_f(x)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head(x)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (b, t, vocab_size)</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">138</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">136 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x: torch.Tensor) -&gt; torch.Tensor:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>138 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rms_1(x))                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rms_2(x))                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">140 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">198</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>device=x.device,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>198 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>q = apply_rope(q, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rope_cache)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>k = apply_rope(k, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rope_cache)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh,</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">292</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_rope</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">289 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># cast because the reference does</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>xshaped = x.float().reshape(*x.shape[:-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>rope_cache = rope_cache.view(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, xshaped.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, xshaped.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>), <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>292 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>x_out2 = torch.stack(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">293 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>[xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] - xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>],      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">294 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │    </span>xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] + xshaped[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] * rope_cache[..., <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>],      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">295 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>], -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m49\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m46 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_loss = \u001b[94m0\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(batch_size):                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m49 \u001b[2m│   │   │   │   \u001b[0m\u001b[96minput\u001b[0m, target = get_single_example(squad_test, index=i)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mllama_output = LLamaModel.forward_embeddings(\u001b[96minput\u001b[0m.type(torch.bfloat16))    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mloss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze(    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mdel\u001b[0m llama_output                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mget_single_example\u001b[0m:\u001b[94m5\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m(index \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m):                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   │   \u001b[0mindex = random.sample(\u001b[96mrange\u001b[0m(\u001b[96mlen\u001b[0m(dataset)), k=\u001b[94m1\u001b[0m)[\u001b[94m0\u001b[0m]                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# IST\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 5 \u001b[2m│   \u001b[0mIST = IST_generator(LLamaModel(dataset[index][\u001b[33m'\u001b[0m\u001b[33minstruction\u001b[0m\u001b[33m'\u001b[0m].unsqueeze(\u001b[94m0\u001b[0m).to(fabric.    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Question\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0mquestion = LLamaModel.transformer.wte(dataset[index][\u001b[33m'\u001b[0m\u001b[33minput\u001b[0m\u001b[33m'\u001b[0m].unsqueeze(\u001b[94m0\u001b[0m).to(fabric    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m88\u001b[0m in \u001b[92mforward\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = torch.cat((internal_state_tokens.reshape(batch_size,\u001b[94m1\u001b[0m,-\u001b[94m1\u001b[0m).to(idx.device)   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 86 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 87 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m block \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.transformer.h:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 88 \u001b[2m│   │   │   \u001b[0mx = block(x)                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.transformer.ln_f(x)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.lm_head(x)  \u001b[2m# (b, t, vocab_size)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m138\u001b[0m in \u001b[92mforward\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x: torch.Tensor) -> torch.Tensor:                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m138 \u001b[2m│   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.attn(\u001b[96mself\u001b[0m.rms_1(x))                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.mlp(\u001b[96mself\u001b[0m.rms_2(x))                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m198\u001b[0m in \u001b[92mforward\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdevice=x.device,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m198 \u001b[2m│   │   \u001b[0mq = apply_rope(q, \u001b[96mself\u001b[0m.rope_cache)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0mk = apply_rope(k, \u001b[96mself\u001b[0m.rope_cache)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh,\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m292\u001b[0m in \u001b[92mapply_rope\u001b[0m                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m289 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# cast because the reference does\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   \u001b[0mxshaped = x.float().reshape(*x.shape[:-\u001b[94m1\u001b[0m], -\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   \u001b[0mrope_cache = rope_cache.view(\u001b[94m1\u001b[0m, xshaped.size(\u001b[94m1\u001b[0m), \u001b[94m1\u001b[0m, xshaped.size(\u001b[94m3\u001b[0m), \u001b[94m2\u001b[0m)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m292 \u001b[2m│   \u001b[0mx_out2 = torch.stack(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m293 \u001b[0m\u001b[2m│   │   \u001b[0m[xshaped[..., \u001b[94m0\u001b[0m] * rope_cache[..., \u001b[94m0\u001b[0m] - xshaped[..., \u001b[94m1\u001b[0m] * rope_cache[..., \u001b[94m1\u001b[0m],      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m294 \u001b[0m\u001b[2m│   │    \u001b[0mxshaped[..., \u001b[94m1\u001b[0m] * rope_cache[..., \u001b[94m0\u001b[0m] + xshaped[..., \u001b[94m0\u001b[0m] * rope_cache[..., \u001b[94m1\u001b[0m],      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m295 \u001b[0m\u001b[2m│   │   \u001b[0m], -\u001b[94m1\u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for param in LLamaModel.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "\n",
    "cumulative_batch_num = 0\n",
    "LLamaModel.eval()\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    indices = list(range(4000))\n",
    "    random.shuffle(indices)\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for batch in range(len(indices) // batch_size):\n",
    "        \n",
    "        IST_generator.train()\n",
    "        wandb_log_dict = {'batch': cumulative_batch_num}\n",
    "        cumulative_batch_num += 1\n",
    "        \n",
    "        batch_indices = indices[:batch_size]\n",
    "        indices = indices[batch_size:]\n",
    "        batch_loss = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for i in range(batch_size):\n",
    "            input, target = get_single_example(squad_train, index=batch_indices[i])\n",
    "            llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "            loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "            del llama_output\n",
    "\n",
    "        batch_loss /= batch_size\n",
    "\n",
    "        optimizer.step()\n",
    "        train_losses.append(batch_loss)\n",
    "        epoch_train_loss += batch_loss\n",
    "        wandb_log_dict['batch train loss'] = batch_loss\n",
    "\n",
    "        # validation:\n",
    "        IST_generator.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_loss = 0\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                input, target = get_single_example(squad_test, index=i)\n",
    "                llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "                loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "                del llama_output\n",
    "                batch_loss += loss.item()\n",
    "            batch_loss /= batch_size\n",
    "\n",
    "            test_losses.append(batch_loss)\n",
    "            wandb_log_dict['batch validation loss'] = batch_loss\n",
    "\n",
    "            if(batch % 10 == 0):\n",
    "                total_f1s = 0\n",
    "                for index, item in enumerate(list(squad['test'])[:10] ):\n",
    "                    context = item['context']\n",
    "                    question = item['question']\n",
    "                    answer = item['answers']['text'][0]\n",
    "                    _, out = generate(LLamaModel, tokenizer, question, IST=get_IST(context),max_new_tokens=len(tokenizer.encode(answer)))\n",
    "                    total_f1s += calculate_F1_score(out, answer)\n",
    "                wandb_log_dict['F1 score'] = total_f1s / 10\n",
    "\n",
    "\n",
    "\n",
    "        print(wandb_log_dict)\n",
    "        \n",
    "        wandb.log(wandb_log_dict)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "his sister and his friends Jane Stirling\n",
      "The 2018-2019 school year is off to a great The Tai Situpa\n",
      "$500,000 $250,000.\n",
      "1869 1883\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>context = item[<span style=\"color: #808000; text-decoration-color: #808000\">'context'</span>]                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>question = item[<span style=\"color: #808000; text-decoration-color: #808000\">'question'</span>]                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>answer = item[<span style=\"color: #808000; text-decoration-color: #808000\">'answers'</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'text'</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 8 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>_, out = generate(LLamaModel, tokenizer, question, IST=get_IST(context),max_new_toke    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#print(out)</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>testing_json.append({<span style=\"color: #808000; text-decoration-color: #808000\">'context'</span>:context, <span style=\"color: #808000; text-decoration-color: #808000\">'question'</span>:question, <span style=\"color: #808000; text-decoration-color: #808000\">'model_output'</span>: out, <span style=\"color: #808000; text-decoration-color: #808000\">'g</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(out, answer)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _ <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(max_new_tokens):                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 8 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>last_logits = model(tokenized_input.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>), IST.type(torch.bfloat16))[    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>new_token = torch.argmax(last_logits, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span>(new_token == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> _ &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>): <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#eos</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">88</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 85 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = torch.cat((internal_state_tokens.reshape(batch_size,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).to(idx.device)   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 86 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 87 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> block <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer.h:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 88 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = block(x)                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer.ln_f(x)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head(x)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (b, t, vocab_size)</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/Documents/lit-llama/lit_llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">138</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">136 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x: torch.Tensor) -&gt; torch.Tensor:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>138 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rms_1(x))                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = x + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rms_2(x))                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">140 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m8\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0mcontext = item[\u001b[33m'\u001b[0m\u001b[33mcontext\u001b[0m\u001b[33m'\u001b[0m]                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0mquestion = item[\u001b[33m'\u001b[0m\u001b[33mquestion\u001b[0m\u001b[33m'\u001b[0m]                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   \u001b[0manswer = item[\u001b[33m'\u001b[0m\u001b[33manswers\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mtext\u001b[0m\u001b[33m'\u001b[0m][\u001b[94m0\u001b[0m]                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 8 \u001b[2m│   \u001b[0m_, out = generate(LLamaModel, tokenizer, question, IST=get_IST(context),max_new_toke    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m#print(out)\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0mtesting_json.append({\u001b[33m'\u001b[0m\u001b[33mcontext\u001b[0m\u001b[33m'\u001b[0m:context, \u001b[33m'\u001b[0m\u001b[33mquestion\u001b[0m\u001b[33m'\u001b[0m:question, \u001b[33m'\u001b[0m\u001b[33mmodel_output\u001b[0m\u001b[33m'\u001b[0m: out, \u001b[33m'\u001b[0m\u001b[33mg\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(out, answer)                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mgenerate\u001b[0m:\u001b[94m8\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m _ \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(max_new_tokens):                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 8 \u001b[2m│   │   │   \u001b[0mlast_logits = model(tokenized_input.unsqueeze(\u001b[94m0\u001b[0m), IST.type(torch.bfloat16))[    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   │   │   \u001b[0mnew_token = torch.argmax(last_logits, dim=\u001b[94m1\u001b[0m)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m(new_token == \u001b[94m2\u001b[0m \u001b[95mand\u001b[0m _ > \u001b[94m0\u001b[0m): \u001b[2m#eos\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m88\u001b[0m in \u001b[92mforward\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 85 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = torch.cat((internal_state_tokens.reshape(batch_size,\u001b[94m1\u001b[0m,-\u001b[94m1\u001b[0m).to(idx.device)   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 86 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 87 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m block \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.transformer.h:                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 88 \u001b[2m│   │   │   \u001b[0mx = block(x)                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.transformer.ln_f(x)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.lm_head(x)  \u001b[2m# (b, t, vocab_size)\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/Documents/lit-llama/lit_llama/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m138\u001b[0m in \u001b[92mforward\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m136 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x: torch.Tensor) -> torch.Tensor:                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m138 \u001b[2m│   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.attn(\u001b[96mself\u001b[0m.rms_1(x))                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   \u001b[0mx = x + \u001b[96mself\u001b[0m.mlp(\u001b[96mself\u001b[0m.rms_2(x))                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/andrew/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testing_json = []\n",
    "exact_match = 0\n",
    "\n",
    "for index, item in enumerate(squad['test']):\n",
    "    context = item['context']\n",
    "    question = item['question']\n",
    "    answer = item['answers']['text'][0]\n",
    "    _, out = generate(LLamaModel, tokenizer, question, IST=get_IST(context),max_new_tokens=20)\n",
    "    #print(out)\n",
    "    testing_json.append({'context':context, 'question':question, 'model_output': out, 'ground_truth': answer})\n",
    "    print(out, answer)\n",
    "    if(out == answer):\n",
    "        exact_match += 1\n",
    "        print('exact match found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "his sister and his friends Jane Stirling\n",
      "The 2018-2019 school year is off to a great The Tai Situpa\n",
      "$500,000 $250,000.\n",
      "1869 1883\n",
      "10,000 135,000\n",
      "October 29, 2012 October 29, 2012\n",
      "exact match found\n",
      "20% 28%\n",
      "The 2019-2020 school year is off to a great Rise Up\n",
      "The 2018-2019 school year is off to a great Health care\n",
      "The 2018-19 school year is off to a great start! not\n",
      "The Funeral March Revolutionary Étude\n",
      "iPod nano (1st generation) Nano\n",
      "2001 2001\n",
      "exact match found\n",
      "Bey Hive Beyontourage\n",
      "\"At the Ballet\" America the Beautiful\n",
      "1830 September 1829\n",
      "1876 1876\n",
      "exact match found\n",
      "10 5,335\n",
      "The 2018-2019 school year is off to a great soldering tools\n",
      "The 2018-2019 school year is off to a great George Clooney and Wyclef Jean\n",
      "jazz\n",
      "What was the name of the first black newspaper in New York? The New York Age\n",
      " jazz\n",
      "St. Mary's College South Bend\n",
      "The 2018-2019 school year is off to a great The British Library\n",
      "Rockefeller Center Rockefeller Center\n",
      "exact match found\n",
      "100 60\n",
      "100,000 10,000\n",
      "100,000 Over 200,000\n",
      "Beijing Nanjing\n",
      "National Park Service National Park Service\n",
      "exact match found\n",
      "Wiimote Wii Remote\n",
      "$1.50 $1.75\n",
      "The 2018-2019 school year is off to a great that which is innocent and harmless\n",
      "1981 1981\n",
      "exact match found\n",
      "Gerhard Domagk Rene Dubos\n",
      "The 2018-2019 school year is off to a great temporal lobe epilepsy.\n",
      "University of Paris Saint Louis University\n",
      "The 2018-2019 school year is off to a great Degsi\n",
      "Truman Capote Truman Capote\n",
      "exact match found\n",
      "The 2018-2019 school year is off to a great 35\n",
      "Irish Germans\n",
      "The 2018-19 season is the 10th season of favorable\n",
      "his relationship with his father bad weather\n",
      "the iPod Camera Connector iPod Hi-Fi\n",
      "The 2018-2019 school year is off to a great Harvard Law School\n",
      "Chime for Change Ban Bossy\n",
      "Massachusetts v. EPA Massachusetts v. Environmental Protection Agency\n",
      "To Kill a Mockingbird Go Set a Watchman\n",
      "music Majorca\n",
      "The 2019-2024 Outlook for Canned and Fro his love life and his early death\n",
      "Pinewood Studios Pinewood Studios\n",
      "exact match found\n",
      "1.5 meters 9 meters\n",
      "The 2018-2019 school year is off to a great biological-physical\n",
      "11 20\n",
      "The 2018-2019 school year is off to a great 10\n",
      "FDA, CDC, and the American Medical Association American Society for Microbiology (ASM), American Public Health Association (APHA) and the American Medical Association (AMA)\n",
      "vaginal yeast infections yeast\n",
      "2008 2015\n",
      "The 2019-2020 school year is off to a great WVFI\n",
      "Beyoncé's Pepsi commercial inspired Jennifer Lopez to join Pepsi Nicki Minaj\n",
      "Ganon beast\n",
      "10,000 10,521.83\n",
      "Dave Bautista Dave Bautista\n",
      "exact match found\n",
      "The 2018-2019 school year is off to a great 200\n",
      "2008 1992\n",
      "25 38.4\n",
      "The 2018-2019 school year is off to a great intuitive\n",
      "The 2019-2024 Outlook for Canned and Fro India\n",
      "The 2018-2019 school year is off to a great 71\n",
      "Roman Catholicism Christianity\n",
      "In 1898, the five boroughs of New York City were combined into 1898\n",
      "Dennis Gassner Dennis Gassner\n",
      "exact match found\n",
      "The 2018-19 school year is off to a great start! iTunes 7\n",
      "The 2018-2019 school year is off to a great 2010\n",
      "piano eolomelodicon\n",
      "Steve Jobs Vinnie Chieco\n",
      "What was the name of Beyonce's second solo album?\n",
      "What was the name of Dangerously in Love\n",
      "The 2019-2020 school year is off to a great nocturne\n",
      "1st third\n",
      "The 2018-2019 school year is off to a great White Rabbits\n",
      "The 2019-2024 Outlook for Canned and Fro several plea bargains\n",
      "1831 1829\n",
      "The size of New York City is 305 square miles.\n",
      "What is the size 305\n",
      "the 1964 Alaska earthquake high risk\n",
      "1909 1909\n",
      "exact match found\n",
      "NBC Sports\n",
      "What is the name of the Notre Dame student newspaper? The Observer\n",
      " NDtv\n",
      "Louis XIV Napoleon\n",
      "Beijing all over mainland China\n",
      "the 2008 earthquake the terrible disaster\n",
      "60 the Convention on the Prevention and Punishment of the Crime of Genocide\n",
      "his mother and sister Feliks Jarocki\n",
      ". Tibet 2. China 3. Mongolia 4. India Ming China\n",
      "20% 20%\n",
      "exact match found\n",
      "The 2018-2019 school year is off to a great Chaillot\n",
      "Destiny's Child Suga Mama\n",
      "The 2018-2019 school year is off to a great William Faulkner\n",
      "Paris Majorca\n",
      "Czerny Elsner\n",
      "10 days 10 days\n",
      "exact match found\n",
      "Two two\n",
      "25% 37%\n"
     ]
    }
   ],
   "source": [
    "testing_json = []\n",
    "exact_match = 0\n",
    "\n",
    "total_f1s = 0\n",
    "for index, item in enumerate(squad['test']):\n",
    "\n",
    "    if(index == 100):\n",
    "        break\n",
    "    context = item['context']\n",
    "    question = item['question']\n",
    "    answer = item['answers']['text'][0]\n",
    "    _, out = generate(LLamaModel, tokenizer, question, IST=get_IST(context),max_new_tokens=20)\n",
    "    #print(out)\n",
    "    testing_json.append({'context':context, 'question':question, 'model_output': out, 'ground_truth': answer})\n",
    "    total_f1s += calculate_F1_score(out, answer)\n",
    "    print(out, answer)\n",
    "    if(out == answer):\n",
    "        exact_match += 1\n",
    "        print('exact match found')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(squad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19050144300144298"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_f1s/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs.json', 'r') as f:\n",
    "    j = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20301104765261133\n"
     ]
    }
   ],
   "source": [
    "total_f1s = 0\n",
    "for item in j:\n",
    "    total_f1s += calculate_F1_score(item['model_output'], item['ground_truth'])\n",
    "\n",
    "print(total_f1s / len(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': \"Chopin's public popularity as a virtuoso began to wane, as did the number of his pupils, and this, together with the political strife and instability of the time, caused him to struggle financially. In February 1848, with the cellist Auguste Franchomme, he gave his last Paris concert, which included three movements of the Cello Sonata Op. 65.\",\n",
       "  'question': 'Who did Chopin have at his last Parisian concert in 1848?',\n",
       "  'model_output': 'Franz Liszt',\n",
       "  'ground_truth': 'Auguste Franchomme'},\n",
       " {'context': 'In 2001, she became the first African-American woman and second woman songwriter to win the Pop Songwriter of the Year award at the American Society of Composers, Authors, and Publishers Pop Music Awards. Beyoncé was the third woman to have writing credits on three number one songs (\"Irreplaceable\", \"Grillz\" and \"Check on It\") in the same year, after Carole King in 1971 and Mariah Carey in 1991. She is tied with American songwriter Diane Warren at third with nine songwriting credits on number-one singles. (The latter wrote her 9/11-motivated song \"I Was Here\" for 4.) In May 2011, Billboard magazine listed Beyoncé at number 17 on their list of the \"Top 20 Hot 100 Songwriters\", for having co-written eight singles that hit number one on the Billboard Hot 100 chart. She was one of only three women on that list.',\n",
       "  'question': 'Pop Songwriter of the Year award in 2001 was awarded to whom?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Beyoncé'},\n",
       " {'context': 'J. Barrie Jones suggests that \"amongst the works that Chopin intended for concert use, the four ballades and four scherzos stand supreme\", and adds that \"the Barcarolle Op. 60 stands apart as an example of Chopin\\'s rich harmonic palette coupled with an Italianate warmth of melody.\" Temperley opines that these works, which contain \"immense variety of mood, thematic material and structural detail\", are based on an extended \"departure and return\" form; \"the more the middle section is extended, and the further it departs in key, mood and theme, from the opening idea, the more important and dramatic is the reprise when it at last comes.\"',\n",
       "  'question': \"What does J. Barrie Jones feel stands supreme of Chopin's concert pieces?\",\n",
       "  'model_output': 'The 2019-2020 school year is off to a great',\n",
       "  'ground_truth': 'the four ballades and four scherzos'},\n",
       " {'context': 'New York City has over 28,000 acres (110 km2) of municipal parkland and 14 miles (23 km) of public beaches. Parks in New York City include Central Park, Prospect Park, Flushing Meadows–Corona Park, Forest Park, and Washington Square Park. The largest municipal park in the city is Pelham Bay Park with 2,700 acres (1,093 ha).',\n",
       "  'question': 'How large is Pelham Bay Park in hectares?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': '1,093'},\n",
       " {'context': \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The plot and characters are loosely based on the author's observations of her family and neighbors, as well as on an event that occurred near her hometown in 1936, when she was 10 years old.\",\n",
       "  'question': 'What prize did To Kill a Mockingbird win?',\n",
       "  'model_output': 'Pulitzer Prize',\n",
       "  'ground_truth': 'Pulitzer Prize'},\n",
       " {'context': \"Returning World War II veterans created a post-war economic boom and the development of large housing tracts in eastern Queens. New York emerged from the war unscathed as the leading city of the world, with Wall Street leading America's place as the world's dominant economic power. The United Nations Headquarters was completed in 1952, solidifying New York's global geopolitical influence, and the rise of abstract expressionism in the city precipitated New York's displacement of Paris as the center of the art world.\",\n",
       "  'question': 'The headquarters what organization was done being build in 1952 in New York?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'United Nations'},\n",
       " {'context': 'In 2001, she became the first African-American woman and second woman songwriter to win the Pop Songwriter of the Year award at the American Society of Composers, Authors, and Publishers Pop Music Awards. Beyoncé was the third woman to have writing credits on three number one songs (\"Irreplaceable\", \"Grillz\" and \"Check on It\") in the same year, after Carole King in 1971 and Mariah Carey in 1991. She is tied with American songwriter Diane Warren at third with nine songwriting credits on number-one singles. (The latter wrote her 9/11-motivated song \"I Was Here\" for 4.) In May 2011, Billboard magazine listed Beyoncé at number 17 on their list of the \"Top 20 Hot 100 Songwriters\", for having co-written eight singles that hit number one on the Billboard Hot 100 chart. She was one of only three women on that list.',\n",
       "  'question': 'Beyonce received the Pop Songwriter of the Year award at which event?',\n",
       "  'model_output': 'BMI Pop Awards',\n",
       "  'ground_truth': 'American Society of Composers, Authors, and Publishers Pop Music Awards'},\n",
       " {'context': 'In 1664, Peter Stuyvesant, the Director-General of the colony of New Netherland, surrendered New Amsterdam to the English without bloodshed. The English promptly renamed the fledgling city \"New York\" after the Duke of York (later King James II).',\n",
       "  'question': 'In what year did the English take over New Amsterdam?',\n",
       "  'model_output': '1664',\n",
       "  'ground_truth': '1664'},\n",
       " {'context': 'In mid-2015, a new model of the iPod Touch was announced by Apple, and was officially released on the Apple store on July 15, 2015. The sixth generation iPod Touch includes a wide variety of spec improvements such as the upgraded A8 processor and higher-quality screen. The core is over 5 times faster than previous models and is built to be roughly on par with the iPhone 5S. It is available in 5 different colors: Space grey, pink, gold, silver and Product (red).',\n",
       "  'question': 'In how many colors is the current iPod Touch available?',\n",
       "  'model_output': '5',\n",
       "  'ground_truth': '5'},\n",
       " {'context': 'There are hundreds of distinct neighborhoods throughout the five boroughs of New York City, many with a definable history and character to call their own. If the boroughs were each independent cities, four of the boroughs (Brooklyn, Queens, Manhattan, and the Bronx) would be among the ten most populous cities in the United States.',\n",
       "  'question': 'How many boroughs does New York City have?',\n",
       "  'model_output': 'five',\n",
       "  'ground_truth': 'five'},\n",
       " {'context': \"All of Chopin's compositions include the piano. Most are for solo piano, though he also wrote two piano concertos, a few chamber pieces, and some songs to Polish lyrics. His keyboard style is highly individual and often technically demanding; his own performances were noted for their nuance and sensitivity. Chopin invented the concept of instrumental ballade. His major piano works also include mazurkas, waltzes, nocturnes, polonaises, études, impromptus, scherzos, preludes and sonatas, some published only after his death. Influences on his compositional style include Polish folk music, the classical tradition of J. S. Bach, Mozart and Schubert, the music of all of whom he admired, as well as the Paris salons where he was a frequent guest. His innovations in style, musical form, and harmony, and his association of music with nationalism, were influential throughout and after the late Romantic period.\",\n",
       "  'question': \"What instrument was all of Chopin's compositions written for?\",\n",
       "  'model_output': 'piano',\n",
       "  'ground_truth': 'piano'},\n",
       " {'context': 'Two Polish friends in Paris were also to play important roles in Chopin\\'s life there. His fellow student at the Warsaw Conservatory, Julian Fontana, had originally tried unsuccessfully to establish himself in England; Albert Grzymała, who in Paris became a wealthy financier and society figure, often acted as Chopin\\'s adviser and \"gradually began to fill the role of elder brother in [his] life.\" Fontana was to become, in the words of Michałowski and Samson, Chopin\\'s \"general factotum and copyist\".',\n",
       "  'question': 'Where did Julian Fontana fail to get established?',\n",
       "  'model_output': 'Paris',\n",
       "  'ground_truth': 'England'},\n",
       " {'context': 'In his usurpation of the throne from the Jianwen Emperor (r. 1398–1402), the Yongle Emperor was aided by the Buddhist monk Yao Guangxiao, and like his father, the Hongwu Emperor, the Yongle Emperor was \"well-disposed towards Buddhism\", claims Rossabi. On March 10, 1403, the Yongle Emperor invited Deshin Shekpa, 5th Karmapa Lama (1384–1415), to his court, even though the fourth Karmapa had rejected the invitation of the Hongwu Emperor. A Tibetan translation in the 16th century preserves the letter of the Yongle Emperor, which the Association for Asian Studies notes is polite and complimentary towards the Karmapa. The letter of invitation reads,',\n",
       "  'question': \"Who was Yongle Emperor's father?\",\n",
       "  'model_output': \"What was the name of the Yongle Emperor's mother?\\nWhat was the name\",\n",
       "  'ground_truth': 'the Hongwu Emperor'},\n",
       " {'context': 'In 1609, English explorer Henry Hudson re-discovered the region when he sailed his ship the Halve Maen (\"Half Moon\" in Dutch) into New York Harbor while searching for the Northwest Passage to the Orient for his employer, the Dutch East India Company. He proceeded to sail up what he named the North River, also called the Mauritis River, and now known as the Hudson River, to the site of the present-day New York State capital of Albany in the belief that it might represent an oceanic tributary. When the river narrowed and was no longer saline, he realized it was not a maritime passage and sailed back downriver. He made a ten-day exploration of the area and claimed the region for his employer. In 1614, the area between Cape Cod and Delaware Bay would be claimed by the Netherlands and called Nieuw-Nederland (New Netherland).',\n",
       "  'question': 'Henry Hudson worked for which company in the 1600s?',\n",
       "  'model_output': 'Dutch East India Company',\n",
       "  'ground_truth': 'Dutch East India Company'},\n",
       " {'context': \"Situated on one of the world's largest natural harbors, New York City consists of five boroughs, each of which is a separate county of New York State. The five boroughs – Brooklyn, Queens, Manhattan, the Bronx, and Staten Island – were consolidated into a single city in 1898. With a census-estimated 2014 population of 8,491,079 distributed over a land area of just 305 square miles (790 km2), New York is the most densely populated major city in the United States. As many as 800 languages are spoken in New York, making it the most linguistically diverse city in the world. By 2014 census estimates, the New York City metropolitan region remains by a significant margin the most populous in the United States, as defined by both the Metropolitan Statistical Area (20.1 million residents) and the Combined Statistical Area (23.6 million residents). In 2013, the MSA produced a gross metropolitan product (GMP) of nearly US$1.39 trillion, while in 2012, the CSA generated a GMP of over US$1.55 trillion, both ranking first nationally by a wide margin and behind the GDP of only twelve and eleven countries, respectively.\",\n",
       "  'question': 'How man boroughs does New York City contain?',\n",
       "  'model_output': 'New York City is made up of five boroughs: Manhattan, Brooklyn, Queens',\n",
       "  'ground_truth': 'five'},\n",
       " {'context': 'In 2015-2016, Notre Dame ranked 18th overall among \"national universities\" in the United States in U.S. News & World Report\\'s Best Colleges 2016. In 2014, USA Today ranked Notre Dame 10th overall for American universities based on data from College Factual. Forbes.com\\'s America\\'s Best Colleges ranks Notre Dame 13th among colleges in the United States in 2015, 8th among Research Universities, and 1st in the Midwest. U.S. News & World Report also lists Notre Dame Law School as 22nd overall. BusinessWeek ranks Mendoza College of Business undergraduate school as 1st overall. It ranks the MBA program as 20th overall. The Philosophical Gourmet Report ranks Notre Dame\\'s graduate philosophy program as 15th nationally, while ARCHITECT Magazine ranked the undergraduate architecture program as 12th nationally. Additionally, the study abroad program ranks sixth in highest participation percentage in the nation, with 57.6% of students choosing to study abroad in 17 countries. According to payscale.com, undergraduate alumni of University of Notre Dame have a mid-career median salary $110,000, making it the 24th highest among colleges and universities in the United States. The median starting salary of $55,300 ranked 58th in the same peer group.',\n",
       "  'question': 'Forbes.com placed Notre Dame at what position compared to other US research universities?',\n",
       "  'model_output': '10th',\n",
       "  'ground_truth': '8th'},\n",
       " {'context': 'Charles Shields, who has written the only book-length biography of Harper Lee to date, offers the reason for the novel\\'s enduring popularity and impact is that \"its lessons of human dignity and respect for others remain fundamental and universal\". Atticus\\' lesson to Scout that \"you never really understand a person until you consider things from his point of view—until you climb around in his skin and walk around in it\" exemplifies his compassion. She ponders the comment when listening to Mayella Ewell\\'s testimony. When Mayella reacts with confusion to Atticus\\' question if she has any friends, Scout offers that she must be lonelier than Boo Radley. Having walked Boo home after he saves their lives, Scout stands on the Radley porch and considers the events of the previous three years from Boo\\'s perspective. One writer remarks, \"... [w]hile the novel concerns tragedy and injustice, heartache and loss, it also carries with it a strong sense [of] courage, compassion, and an awareness of history to be better human beings.\"',\n",
       "  'question': 'Who does Scout think could be lonelier than Boo Radley?',\n",
       "  'model_output': 'The 2019-2020 school year is off to a great',\n",
       "  'ground_truth': 'Mayella Ewell'},\n",
       " {'context': 'Van Praag states that the Ming court established diplomatic delegations with Tibet merely to secure urgently needed horses. Wang and Nyima argue that these were not diplomatic delegations at all, that Tibetan areas were ruled by the Ming since Tibetan leaders were granted positions as Ming officials, that horses were collected from Tibet as a mandatory \"corvée\" tax, and therefore Tibetans were \"undertaking domestic affairs, not foreign diplomacy\". Sperling writes that the Ming simultaneously bought horses in the Kham region while fighting Tibetan tribes in Amdo and receiving Tibetan embassies in Nanjing. He also argues that the embassies of Tibetan lamas visiting the Ming court were for the most part efforts to promote commercial transactions between the lamas\\' large, wealthy entourage and Ming Chinese merchants and officials. Kolmaš writes that while the Ming maintained a laissez-faire policy towards Tibet and limited the numbers of the Tibetan retinues, the Tibetans sought to maintain a tributary relationship with the Ming because imperial patronage provided them with wealth and power. Laird writes that Tibetans eagerly sought Ming court invitations since the gifts the Tibetans received for bringing tribute were much greater in value than the latter. As for the Yongle Emperor\\'s gifts to his Tibetan and Nepalese vassals such as silver wares, Buddha relics, utensils for Buddhist temples and religious ceremonies, and gowns and robes for monks, Tsai writes \"in his effort to draw neighboring states to the Ming orbit so that he could bask in glory, the Yongle Emperor was quite willing to pay a small price\". The Information Office of the State Council of the PRC lists the Tibetan tribute items as oxen, horses, camels, sheep, fur products, medical herbs, Tibetan incenses, thangkas (painted scrolls), and handicrafts; while the Ming awarded Tibetan tribute-bearers an equal value of gold, silver, satin and brocade, bolts of cloth, grains, and tea leaves. Silk workshops during the Ming also catered specifically to the Tibetan market with silk clothes and furnishings featuring Tibetan Buddhist iconography.',\n",
       "  'question': 'What visual images and symbols were on furnishings from the silk workshops?',\n",
       "  'model_output': 'The 2019-2020 school year is off to a great',\n",
       "  'ground_truth': 'Tibetan Buddhist'},\n",
       " {'context': 'In order to seek out the Karmapa, the Yongle Emperor dispatched his eunuch Hou Xian and the Buddhist monk Zhi Guang (d. 1435) to Tibet. Traveling to Lhasa either through Qinghai or via the Silk Road to Khotan, Hou Xian and Zhi Guang did not return to Nanjing until 1407.',\n",
       "  'question': 'Who did the Yongle Emperor send to Tibet?',\n",
       "  'model_output': 'Wang Chong-yang',\n",
       "  'ground_truth': 'Hou Xian and the Buddhist monk Zhi Guang'},\n",
       " {'context': 'New York has been described as the \"Capital of Baseball\". There have been 35 Major League Baseball World Series and 73 pennants won by New York teams. It is one of only five metro areas (Los Angeles, Chicago, Baltimore–Washington, and the San Francisco Bay Area being the others) to have two baseball teams. Additionally, there have been 14 World Series in which two New York City teams played each other, known as a Subway Series and occurring most recently in 2000. No other metropolitan area has had this happen more than once (Chicago in 1906, St. Louis in 1944, and the San Francisco Bay Area in 1989). The city\\'s two current Major League Baseball teams are the New York Mets, who play at Citi Field in Queens, and the New York Yankees, who play at Yankee Stadium in the Bronx. who compete in six games of interleague play every regular season that has also come to be called the Subway Series. The Yankees have won a record 27 championships, while the Mets have won the World Series twice. The city also was once home to the Brooklyn Dodgers (now the Los Angeles Dodgers), who won the World Series once, and the New York Giants (now the San Francisco Giants), who won the World Series five times. Both teams moved to California in 1958. There are also two Minor League Baseball teams in the city, the Brooklyn Cyclones and Staten Island Yankees.',\n",
       "  'question': 'How many times have two teams from New York played against each other in the World Series?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': '14'},\n",
       " {'context': \"Fryderyk's father, Nicolas Chopin, was a Frenchman from Lorraine who had emigrated to Poland in 1787 at the age of sixteen. Nicolas tutored children of the Polish aristocracy, and in 1806 married Justyna Krzyżanowska, a poor relative of the Skarbeks, one of the families for whom he worked. Fryderyk was baptized on Easter Sunday, 23 April 1810, in the same church where his parents had married, in Brochów. His eighteen-year-old godfather, for whom he was named, was Fryderyk Skarbek, a pupil of Nicolas Chopin. Fryderyk was the couple's second child and only son; he had an elder sister, Ludwika (1807–55), and two younger sisters, Izabela (1811–81) and Emilia (1812–27). Nicolas was devoted to his adopted homeland, and insisted on the use of the Polish language in the household.\",\n",
       "  'question': \"Who did Frédéric's father marry in 1806?\",\n",
       "  'model_output': 'a Polish countess',\n",
       "  'ground_truth': 'Justyna Krzyżanowska'},\n",
       " {'context': \"Chopin's life was covered in a BBC TV documentary Chopin – The Women Behind The Music (2010), and in a 2010 documentary realised by Angelo Bozzolini and Roberto Prosseda for Italian television.\",\n",
       "  'question': 'What television station made a documentary on Chopin?',\n",
       "  'model_output': 'BBC',\n",
       "  'ground_truth': 'BBC'},\n",
       " {'context': 'A high-definition remaster of the game, The Legend of Zelda: Twilight Princess HD, is being developed by Tantalus Media for the Wii U. Officially announced during a Nintendo Direct presentation on November 12, 2015, it features enhanced graphics and Amiibo functionality. The game will be released in North America and Europe on March 4, 2016; in Australia on March 5, 2016; and in Japan on March 10, 2016.',\n",
       "  'question': 'Which company is responsible for the HD version of Twilight Princess?',\n",
       "  'model_output': 'Nintendo',\n",
       "  'ground_truth': 'Tantalus Media'},\n",
       " {'context': \"The game's score was composed by Toru Minegishi and Asuka Ohta, with series regular Koji Kondo serving as the sound supervisor. Minegishi took charge of composition and sound design in Twilight Princess, providing all field and dungeon music under the supervision of Kondo. For the trailers, three pieces were written by different composers, two of which were created by Mahito Yokota and Kondo. Michiru Ōshima created orchestral arrangements for the three compositions, later to be performed by an ensemble conducted by Yasuzo Takemoto. Kondo's piece was later chosen as music for the E3 2005 trailer and for the demo movie after the game's title screen.\",\n",
       "  'question': 'Who was to conduct the ensemble that would perform the pieces?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Yasuzo Takemoto'},\n",
       " {'context': 'The word genocide was later included as a descriptive term to the process of indictment, but not yet as a formal legal term According to Lemming, genocide was defined as \"a coordinated strategy to destroy a group of people, a process that could be accomplished through total annihilation as well as strategies that eliminate key elements of the group\\'s basic existence, including language, culture, and economic infrastructure.” He created a concept of mobilizing much of the international relations and community, to working together and preventing the occurrence of such events happening within history and the international society. Australian anthropologist Peg LeVine coined the term \"ritualcide\" to describe the destruction of a group\\'s cultural identity without necessarily destroying its members.',\n",
       "  'question': 'What elements of group existence, other than people themselves, can be targets of genocide?',\n",
       "  'model_output': 'The 2019-2024 Outlook for Canned and Fro',\n",
       "  'ground_truth': 'language, culture, and economic infrastructure'},\n",
       " {'context': 'iPod batteries are not designed to be removed or replaced by the user, although some users have been able to open the case themselves, usually following instructions from third-party vendors of iPod replacement batteries. Compounding the problem, Apple initially would not replace worn-out batteries. The official policy was that the customer should buy a refurbished replacement iPod, at a cost almost equivalent to a brand new one. All lithium-ion batteries lose capacity during their lifetime even when not in use (guidelines are available for prolonging life-span) and this situation led to a market for third-party battery replacement kits.',\n",
       "  'question': 'What kind of battery does the iPod use?',\n",
       "  'model_output': 'lithium ion',\n",
       "  'ground_truth': 'lithium-ion'},\n",
       " {'context': 'The Internet was extensively used for passing information to aid rescue and recovery efforts. For example, the official news agency Xinhua set up an online rescue request center in order to find the blind spots of disaster recovery. After knowing that rescue helicopters had trouble landing into the epicenter area in Wenchuan, a student proposed a landing spot online and it was chosen as the first touchdown place for the helicopters[not in citation given]. Volunteers also set up several websites to help store contact information for victims and evacuees. On May 31, a rescue helicopter carrying earthquake survivors and crew members crashed in fog and turbulence in Wenchuan county. No-one survived.',\n",
       "  'question': 'What person suggested a landing spot for helicopters near the epicenter?',\n",
       "  'model_output': 'Zhang Qingwei',\n",
       "  'ground_truth': 'a student'},\n",
       " {'context': 'Songbirds and their associated symbolism appear throughout the novel. The family\\'s last name of Finch also shares Lee\\'s mother\\'s maiden name. The titular mockingbird is a key motif of this theme, which first appears when Atticus, having given his children air-rifles for Christmas, allows their Uncle Jack to teach them to shoot. Atticus warns them that, although they can \"shoot all the bluejays they want\", they must remember that \"it\\'s a sin to kill a mockingbird\". Confused, Scout approaches her neighbor Miss Maudie, who explains that mockingbirds never harm other living creatures. She points out that mockingbirds simply provide pleasure with their songs, saying, \"They don\\'t do one thing but sing their hearts out for us.\" Writer Edwin Bruell summarized the symbolism when he wrote in 1964, \"\\'To kill a mockingbird\\' is to kill that which is innocent and harmless—like Tom Robinson.\" Scholars have noted that Lee often returns to the mockingbird theme when trying to make a moral point.',\n",
       "  'question': 'According to Miss Maudie, which bird is never harmful?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'mockingbird'},\n",
       " {'context': 'The Democratic Party holds the majority of public offices. As of November 2008, 67% of registered voters in the city are Democrats. New York City has not been carried by a Republican in a statewide or presidential election since President Calvin Coolidge won the five boroughs in 1924. In 2012, Democrat Barack Obama became the first presidential candidate of any party to receive more than 80% of the overall vote in New York City, sweeping all five boroughs. Party platforms center on affordable housing, education, and economic development, and labor politics are of importance in the city.',\n",
       "  'question': 'What was the last year that a republican candidate won all four boroughs of NYC?',\n",
       "  'model_output': '1989',\n",
       "  'ground_truth': '1924'},\n",
       " {'context': 'Solar cookers use sunlight for cooking, drying and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of 90–150 °C (194–302 °F). Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of 315 °C (599 °F) and above but require direct light to function properly and must be repositioned to track the Sun.',\n",
       "  'question': 'Horace de Saussure built the first box cooker in what year?',\n",
       "  'model_output': '1767',\n",
       "  'ground_truth': '1767'},\n",
       " {'context': 'In 1609, English explorer Henry Hudson re-discovered the region when he sailed his ship the Halve Maen (\"Half Moon\" in Dutch) into New York Harbor while searching for the Northwest Passage to the Orient for his employer, the Dutch East India Company. He proceeded to sail up what he named the North River, also called the Mauritis River, and now known as the Hudson River, to the site of the present-day New York State capital of Albany in the belief that it might represent an oceanic tributary. When the river narrowed and was no longer saline, he realized it was not a maritime passage and sailed back downriver. He made a ten-day exploration of the area and claimed the region for his employer. In 1614, the area between Cape Cod and Delaware Bay would be claimed by the Netherlands and called Nieuw-Nederland (New Netherland).',\n",
       "  'question': 'What did Henry Hudson call the river that is now called the Hudson River?',\n",
       "  'model_output': 'Mauritius River',\n",
       "  'ground_truth': 'North River'},\n",
       " {'context': 'On 3 December, Chopin complained about his bad health and the incompetence of the doctors in Majorca: \"Three doctors have visited me ... The first said I was dead; the second said I was dying; and the third said I was about to die.\" He also had problems having his Pleyel piano sent to him. It finally arrived from Paris in December. Chopin wrote to Pleyel in January 1839: \"I am sending you my Preludes [(Op. 28)]. I finished them on your little piano, which arrived in the best possible condition in spite of the sea, the bad weather and the Palma customs.\" Chopin was also able to undertake work on his Ballade No. 2, Op. 38; two Polonaises, Op. 40; and the Scherzo No. 3, Op. 39.',\n",
       "  'question': 'What did Chopin have a hard time getting delivered to Majorca?',\n",
       "  'model_output': 'a piano',\n",
       "  'ground_truth': 'his Pleyel piano'},\n",
       " {'context': 'At the end of 1831, Chopin received the first major endorsement from an outstanding contemporary when Robert Schumann, reviewing the Op. 2 Variations in the Allgemeine musikalische Zeitung (his first published article on music), declared: \"Hats off, gentlemen! A genius.\" On 26 February 1832 Chopin gave a debut Paris concert at the Salle Pleyel which drew universal admiration. The critic François-Joseph Fétis wrote in the Revue et gazette musicale: \"Here is a young man who ... taking no model, has found, if not a complete renewal of piano music, ... an abundance of original ideas of a kind to be found nowhere else ...\" After this concert, Chopin realized that his essentially intimate keyboard technique was not optimal for large concert spaces. Later that year he was introduced to the wealthy Rothschild banking family, whose patronage also opened doors for him to other private salons (social gatherings of the aristocracy and artistic and literary elite). By the end of 1832 Chopin had established himself among the Parisian musical elite, and had earned the respect of his peers such as Hiller, Liszt, and Berlioz. He no longer depended financially upon his father, and in the winter of 1832 he began earning a handsome income from publishing his works and teaching piano to affluent students from all over Europe. This freed him from the strains of public concert-giving, which he disliked.',\n",
       "  'question': 'Who gave Frédéric his first significant public approval in regards to his compositions?',\n",
       "  'model_output': 'Franz Liszt',\n",
       "  'ground_truth': 'Robert Schumann'},\n",
       " {'context': \"In March 2013 Mendes said he would not return to direct the next film in the series, then known as Bond 24; he later recanted and announced that he would return, as he found the script and the plans for the long-term future of the franchise appealing. In directing Skyfall and Spectre, Mendes became the first director to oversee two consecutive Bond films since John Glen directed The Living Daylights and Licence to Kill in 1987 and 1989. Skyfall writer John Logan resumed his role of scriptwriter, collaborating with Neal Purvis and Robert Wade, who returned for their sixth Bond film.[N 4] The writer Jez Butterworth also worked on the script, alongside Mendes and Craig. Dennis Gassner returned as the film's production designer, while cinematographer Hoyte van Hoytema took over from Roger Deakins. In July 2015 Mendes noted that the combined crew of Spectre numbered over one thousand, making it a larger production than Skyfall. Craig is listed as co-producer.\",\n",
       "  'question': 'Neal Purvis and Robert Wade have worked on how many Bond films?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'six'},\n",
       " {'context': \"By May 15, Premier Wen Jiabao ordered the deployment of an additional 90 helicopters, of which 60 were to be provided by the PLAAF, and 30 were to be provided by the civil aviation industry, bringing the total of number of aircraft deployed in relief operations by the air force, army, and civil aviation to over 150, resulting in the largest non-combat airlifting operation in People's Liberation Army history.\",\n",
       "  'question': 'What did Premier Wen Jiabao order?',\n",
       "  'model_output': '100,000 troops to be sent to the quake zone\\nWhat did',\n",
       "  'ground_truth': 'the deployment of an additional 90 helicopters'},\n",
       " {'context': 'In mid-2015, several new color schemes for all of the current iPod models were spotted in the latest version of iTunes, 12.2. Belgian website Belgium iPhone originally found the images when plugging in an iPod for the first time, and subsequent leaked photos were found by Pierre Dandumont.',\n",
       "  'question': 'What was the latest version of iTunes as of mid-2015?',\n",
       "  'model_output': '12.2',\n",
       "  'ground_truth': '12.2'},\n",
       " {'context': 'Further references to Fleming\\'s material can be found throughout the film; an MI6 safehouse is called \"Hildebrand Rarities and Antiques\", a reference to the short story \"The Hildebrand Rarity\" from the For Your Eyes Only short story collection.[citation needed] Bond\\'s torture by Blofeld mirrors his torture by the title character of Kingsley Amis\\' continuation novel Colonel Sun.[citation needed]',\n",
       "  'question': 'What is the name of the MI6 safehouse?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Hildebrand Rarities and Antiques'},\n",
       " {'context': 'In August, the couple attended the 2011 MTV Video Music Awards, at which Beyoncé performed \"Love on Top\" and started the performance saying \"Tonight I want you to stand up on your feet, I want you to feel the love that\\'s growing inside of me\". At the end of the performance, she dropped her microphone, unbuttoned her blazer and rubbed her stomach, confirming her pregnancy she had alluded to earlier in the evening. Her appearance helped that year\\'s MTV Video Music Awards become the most-watched broadcast in MTV history, pulling in 12.4 million viewers; the announcement was listed in Guinness World Records for \"most tweets per second recorded for a single event\" on Twitter, receiving 8,868 tweets per second and \"Beyonce pregnant\" was the most Googled term the week of August 29, 2011.',\n",
       "  'question': 'How many people watched the 2011 MTV Video Music Awards?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': '12.4 million'},\n",
       " {'context': 'In 2001, she became the first African-American woman and second woman songwriter to win the Pop Songwriter of the Year award at the American Society of Composers, Authors, and Publishers Pop Music Awards. Beyoncé was the third woman to have writing credits on three number one songs (\"Irreplaceable\", \"Grillz\" and \"Check on It\") in the same year, after Carole King in 1971 and Mariah Carey in 1991. She is tied with American songwriter Diane Warren at third with nine songwriting credits on number-one singles. (The latter wrote her 9/11-motivated song \"I Was Here\" for 4.) In May 2011, Billboard magazine listed Beyoncé at number 17 on their list of the \"Top 20 Hot 100 Songwriters\", for having co-written eight singles that hit number one on the Billboard Hot 100 chart. She was one of only three women on that list.',\n",
       "  'question': 'Who listed her at number 17 in their list of  Top 20 hot 100 Songwriters?',\n",
       "  'model_output': 'Beyonce',\n",
       "  'ground_truth': 'Billboard magazine'},\n",
       " {'context': 'Beyoncé\\'s vocal range spans four octaves. Jody Rosen highlights her tone and timbre as particularly distinctive, describing her voice as \"one of the most compelling instruments in popular music\". While another critic says she is a \"Vocal acrobat, being able to sing long and complex melismas and vocal runs effortlessly, and in key. Her vocal abilities mean she is identified as the centerpiece of Destiny\\'s Child. The Daily Mail calls Beyoncé\\'s voice \"versatile\", capable of exploring power ballads, soul, rock belting, operatic flourishes, and hip hop. Jon Pareles of The New York Times commented that her voice is \"velvety yet tart, with an insistent flutter and reserves of soul belting\". Rosen notes that the hip hop era highly influenced Beyoncé\\'s strange rhythmic vocal style, but also finds her quite traditionalist in her use of balladry, gospel and falsetto. Other critics praise her range and power, with Chris Richards of The Washington Post saying she was \"capable of punctuating any beat with goose-bump-inducing whispers or full-bore diva-roars.\"',\n",
       "  'question': \"How many octaves does Beyoncé's voice span?\",\n",
       "  'model_output': '3 1/2',\n",
       "  'ground_truth': 'four'},\n",
       " {'context': 'In 1817 the Saxon Palace was requisitioned by Warsaw\\'s Russian governor for military use, and the Warsaw Lyceum was reestablished in the Kazimierz Palace (today the rectorate of Warsaw University). Fryderyk and his family moved to a building, which still survives, adjacent to the Kazimierz Palace. During this period, Fryderyk was sometimes invited to the Belweder Palace as playmate to the son of the ruler of Russian Poland, Grand Duke Constantine; he played the piano for the Duke and composed a march for him. Julian Ursyn Niemcewicz, in his dramatic eclogue, \"Nasze Przebiegi\" (\"Our Discourses\", 1818), attested to \"little Chopin\\'s\" popularity.',\n",
       "  'question': \"What type of musical piece did Chopin compose for his friend's ruling father?\",\n",
       "  'model_output': 'mazurka',\n",
       "  'ground_truth': 'a march'},\n",
       " {'context': 'Persistent heavy rain and landslides in Wenchuan County and the nearby area badly affected rescue efforts. At the start of rescue operations on May 12, 20 helicopters were deployed for the delivery of food, water, and emergency aid, and also the evacuation of the injured and reconnaissance of quake-stricken areas. By 17:37 CST on May 13, a total of over 15,600 troops and militia reservists from the Chengdu Military Region had joined the rescue force in the heavily affected areas. A commander reported from Yingxiu Town, Wenchuan, that around 3,000 survivors were found, while the status of the other inhabitants (around 9,000) remained unclear. The 1,300 rescuers reached the epicenter, and 300 pioneer troops reached the seat of Wenchuan at about 23:30 CST. By 12:17 CST, May 14, 2008, communication in the seat of Wenchuan was partly revived. On the afternoon of May 14, 15 Special Operations Troops, along with relief supplies and communications gear, parachuted into inaccessible Mao County, northeast of Wenchuan.',\n",
       "  'question': 'How many helicopters were sent to deliver aid to the affected areas?',\n",
       "  'model_output': '10',\n",
       "  'ground_truth': '20'},\n",
       " {'context': \"Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus run by the Congregation of Holy Cross. The current Basilica of the Sacred Heart is located on the spot of Fr. Sorin's original church, which became too small for the growing college. It is built in French Revival style and it is decorated by stained glass windows imported directly from France. The interior was painted by Luigi Gregori, an Italian painter invited by Fr. Sorin to be artist in residence. The Basilica also features a bell tower with a carillon. Inside the church there are also sculptures by Ivan Mestrovic. The Grotto of Our Lady of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France. It is very popular among students and alumni as a place of prayer and meditation, and it is considered one of the most beloved spots on campus.\",\n",
       "  'question': 'What structure is found on the location of the original church of Father Sorin at Notre Dame?',\n",
       "  'model_output': 'Basilica of the Sacred Heart',\n",
       "  'ground_truth': 'Basilica of the Sacred Heart'},\n",
       " {'context': \"In 1904, the steamship General Slocum caught fire in the East River, killing 1,021 people on board. In 1911, the Triangle Shirtwaist Factory fire, the city's worst industrial disaster, took the lives of 146 garment workers and spurred the growth of the International Ladies' Garment Workers' Union and major improvements in factory safety standards.\",\n",
       "  'question': 'In what year did the General Slocum disaster occur?',\n",
       "  'model_output': '1904',\n",
       "  'ground_truth': '1904'},\n",
       " {'context': 'The first documented visit by a European was in 1524 by Giovanni da Verrazzano, a Florentine explorer in the service of the French crown, who sailed his ship La Dauphine into New York Harbor. He claimed the area for France and named it \"Nouvelle Angoulême\" (New Angoulême).',\n",
       "  'question': 'What did Giovanni da Verrazzano call the area when he staked claim on it?',\n",
       "  'model_output': 'New Angoulême',\n",
       "  'ground_truth': 'Nouvelle Angoulême'},\n",
       " {'context': 'In Paris, Chopin encountered artists and other distinguished figures, and found many opportunities to exercise his talents and achieve celebrity. During his years in Paris he was to become acquainted with, among many others, Hector Berlioz, Franz Liszt, Ferdinand Hiller, Heinrich Heine, Eugène Delacroix, and Alfred de Vigny. Chopin was also acquainted with the poet Adam Mickiewicz, principal of the Polish Literary Society, some of whose verses he set as songs.',\n",
       "  'question': 'What position did the poet who Chopin knew in Paris hold?',\n",
       "  'model_output': 'a French consul',\n",
       "  'ground_truth': 'principal of the Polish Literary Society'},\n",
       " {'context': \"Chopin's tombstone, featuring the muse of music, Euterpe, weeping over a broken lyre, was designed and sculpted by Clésinger. The expenses of the funeral and monument, amounting to 5,000 francs, were covered by Jane Stirling, who also paid for the return of the composer's sister Ludwika to Warsaw. Ludwika took Chopin's heart in an urn, preserved in alcohol, back to Poland in 1850.[n 9] She also took a collection of two hundred letters from Sand to Chopin; after 1851 these were returned to Sand, who seems to have destroyed them.\",\n",
       "  'question': 'Who ended up with the 200 letters from Sand to Chopin?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Sand'},\n",
       " {'context': \"The annual United States Open Tennis Championships is one of the world's four Grand Slam tennis tournaments and is held at the National Tennis Center in Flushing Meadows-Corona Park, Queens. The New York Marathon is one of the world's largest, and the 2004–2006 events hold the top three places in the marathons with the largest number of finishers, including 37,866 finishers in 2006. The Millrose Games is an annual track and field meet whose featured event is the Wanamaker Mile. Boxing is also a prominent part of the city's sporting scene, with events like the Amateur Boxing Golden Gloves being held at Madison Square Garden each year. The city is also considered the host of the Belmont Stakes, the last, longest and oldest of horse racing's Triple Crown races, held just over the city's border at Belmont Park on the first or second Sunday of June. The city also hosted the 1932 U.S. Open golf tournament and the 1930 and 1939 PGA Championships, and has been host city for both events several times, most notably for nearby Winged Foot Golf Club.\",\n",
       "  'question': 'What tennis Grand Slam event is held at the National Tennis Center?',\n",
       "  'model_output': 'U.S. Open',\n",
       "  'ground_truth': 'United States Open Tennis Championships'},\n",
       " {'context': \"Thomas Newman returned as Spectre's composer. Rather than composing the score once the film had moved into post-production, Newman worked during filming. The theatrical trailer released in July 2015 contained a rendition of John Barry's On Her Majesty's Secret Service theme. Mendes revealed that the final film would have more than one hundred minutes of music. The soundtrack album was released on 23 October 2015 in the UK and 6 November 2015 in the USA on the Decca Records label.\",\n",
       "  'question': 'What company published the Spectre soundtrack?',\n",
       "  'model_output': 'Sony Classical',\n",
       "  'ground_truth': 'Decca Records'},\n",
       " {'context': \"Foxconn, Apple's manufacturer, initially denied the abuses, but when an auditing team from Apple found that workers had been working longer hours than were allowed under Chinese law, they promised to prevent workers working more hours than the code allowed. Apple hired a workplace standards auditing company, Verité, and joined the Electronic Industry Code of Conduct Implementation Group to oversee the measures. On December 31, 2006, workers at the Foxconn factory in Longhua, Shenzhen formed a union affiliated with the All-China Federation of Trade Unions, the Chinese government-approved union umbrella organization.\",\n",
       "  'question': 'What auditor company did Apple hire to oversee worker conditions following the controversy?',\n",
       "  'model_output': 'Deloitte & Touche',\n",
       "  'ground_truth': 'Verité'},\n",
       " {'context': 'Beyoncé has been described as a having a wide-ranging sex appeal, with music journalist Touré writing that since the release of Dangerously in Love, she has \"become a crossover sex symbol\". Offstage Beyoncé says that while she likes to dress sexily, her onstage dress \"is absolutely for the stage.\" Due to her curves and the term\\'s catchiness, in the 2000s, the media often used the term \"Bootylicious\" (a portmanteau of the words booty and delicious) to describe Beyoncé, the term popularized by Destiny\\'s Child\\'s single of the same name. In 2006, it was added to the Oxford English Dictionary.',\n",
       "  'question': \"Because of Beyoncé's physical shape, what slang term has been used to describe her?\",\n",
       "  'model_output': 'Beyonce booty',\n",
       "  'ground_truth': 'Bootylicious'},\n",
       " {'context': 'The emergence of resistance of bacteria to antibiotics is a common phenomenon. Emergence of resistance often reflects evolutionary processes that take place during antibiotic therapy. The antibiotic treatment may select for bacterial strains with physiologically or genetically enhanced capacity to survive high doses of antibiotics. Under certain conditions, it may result in preferential growth of resistant bacteria, while growth of susceptible bacteria is inhibited by the drug. For example, antibacterial selection for strains having previously acquired antibacterial-resistance genes was demonstrated in 1943 by the Luria–Delbrück experiment. Antibiotics such as penicillin and erythromycin, which used to have a high efficacy against many bacterial species and strains, have become less effective, due to the increased resistance of many bacterial strains.',\n",
       "  'question': 'When was antibacterial-resistance demonstrated?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': '1943'},\n",
       " {'context': \"Christoph Waltz was cast in the role of Franz Oberhauser, though he refused to comment on the nature of the part. It was later revealed with the film's release that he is Ernst Stavro Blofeld. Dave Bautista was cast as Mr. Hinx after producers sought an actor with a background in contact sports. After casting Bérénice Lim Marlohe, a relative newcomer, as Sévérine in Skyfall, Mendes consciously sought out a more experienced actor for the role of Madeleine Swann, ultimately casting Léa Seydoux in the role. Monica Bellucci joined the cast as Lucia Sciarra, becoming, at the age of fifty, the oldest actress to be cast as a Bond girl. In a separate interview with Danish website Euroman, Jesper Christensen revealed he would be reprising his role as Mr. White from Casino Royale and Quantum of Solace. Christensen's character was reportedly killed off in a scene intended to be used as an epilogue to Quantum of Solace, before it was removed from the final cut of the film, enabling his return in Spectre.\",\n",
       "  'question': 'Which actress was cast in the role of Severine in Skyfall?',\n",
       "  'model_output': 'Berenice Marlohe',\n",
       "  'ground_truth': 'Bérénice Lim Marlohe'},\n",
       " {'context': 'Before the early 20th century, treatments for infections were based primarily on medicinal folklore. Mixtures with antimicrobial properties that were used in treatments of infections were described over 2000 years ago. Many ancient cultures, including the ancient Egyptians and ancient Greeks, used specially selected mold and plant materials and extracts to treat infections. More recent observations made in the laboratory of antibiosis between microorganisms led to the discovery of natural antibacterials produced by microorganisms. Louis Pasteur observed, \"if we could intervene in the antagonism observed between some bacteria, it would offer perhaps the greatest hopes for therapeutics\". The term \\'antibiosis\\', meaning \"against life\", was introduced by the French bacteriologist Jean Paul Vuillemin as a descriptive name of the phenomenon exhibited by these early antibacterial drugs. Antibiosis was first described in 1877 in bacteria when Louis Pasteur and Robert Koch observed that an airborne bacillus could inhibit the growth of Bacillus anthracis. These drugs were later renamed antibiotics by Selman Waksman, an American microbiologist, in 1942. Synthetic antibiotic chemotherapy as a science and development of antibacterials began in Germany with Paul Ehrlich in the late 1880s. Ehrlich noted certain dyes would color human, animal, or bacterial cells, whereas others did not. He then proposed the idea that it might be possible to create chemicals that would act as a selective drug that would bind to and kill bacteria without harming the human host. After screening hundreds of dyes against various organisms, in 1907, he discovered a medicinally useful drug, the synthetic antibacterial salvarsan now called arsphenamine.',\n",
       "  'question': 'Who came up with the term antibiosis?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Jean Paul Vuillemin'},\n",
       " {'context': 'Spectre has received mixed reviews, with many reviewers either giving the film highly positive or highly negative feedback. Many critics praised the film\\'s opening scene, action sequences, stuntwork, cinematography and performances from the cast. In some early reviews, the film received favourable comparisons with its predecessor, Skyfall. Rotten Tomatoes sampled 274 reviews and judged 64% of the critiques to be positive, saying that the film \"nudges Daniel Craig\\'s rebooted Bond closer to the glorious, action-driven spectacle of earlier entries, although it\\'s admittedly reliant on established 007 formula.\" On Metacritic, the film has a rating of 60 out of 100, based on 48 critics, indicating \"mixed or average reviews\". Audiences polled by CinemaScore gave the film an average grade of \"A−\" on an A+ to F scale.',\n",
       "  'question': \"What was the movie's rating on Metacritic?\",\n",
       "  'model_output': '75',\n",
       "  'ground_truth': '60 out of 100'},\n",
       " {'context': 'Marsha Weidner states that Deshin Shekpa\\'s miracles \"testified to the power of both the emperor and his guru and served as a legitimizing tool for the emperor\\'s problematic succession to the throne,\" referring to the Yongle Emperor\\'s conflict with the previous Jianwen Emperor. Tsai writes that Deshin Shekpa aided the legitimacy of the Yongle Emperor\\'s rule by providing him with portents and omens which demonstrated Heaven\\'s favor of the Yongle Emperor on the Ming throne.',\n",
       "  'question': 'Who did the Yongle Emperor have a conflict with?',\n",
       "  'model_output': 'the Mongols',\n",
       "  'ground_truth': 'Jianwen Emperor'},\n",
       " {'context': 'The New York Islanders and the New York Rangers represent the city in the National Hockey League. Also within the metropolitan area are the New Jersey Devils, who play in nearby Newark, New Jersey.',\n",
       "  'question': 'Along with the New York Rangers, what NHL franchise is based in New York?',\n",
       "  'model_output': 'New York Islanders',\n",
       "  'ground_truth': 'New York Islanders'},\n",
       " {'context': 'According to a study by the China Earthquake Administration (CEA), the earthquake occurred along the Longmenshan fault, a thrust structure along the border of the Indo-Australian Plate and Eurasian Plate. Seismic activities concentrated on its mid-fracture (known as Yingxiu-Beichuan fracture). The rupture lasted close to 120 sec, with the majority of energy released in the first 80 sec. Starting from Wenchuan, the rupture propagated at an average speed of 3.1 kilometers per second 49° toward north east, rupturing a total of about 300 km. Maximum displacement amounted to 9 meters. The focus was deeper than 10 km.',\n",
       "  'question': 'How long did the rupture last?',\n",
       "  'model_output': '100 seconds\\nHow long did the rupture last? 100 seconds',\n",
       "  'ground_truth': '120 sec'},\n",
       " {'context': \"Strong aftershocks continued to strike even months after the main quake. On May 25, an aftershock of 6.0 Mw (6.4 Ms according to CEA) hit northeast of the original earthquake's epicenter, in Qingchuan County, Sichuan, causing eight deaths, 1000 injuries, and destroying thousands of buildings. On May 27, two aftershocks, one 5.2 Mw in Qingchuan County and one 5.7 Mw in Ningqiang County, Shaanxi, led to the collapse of more than 420,000 homes and injured 63 people. The same area suffered two more aftershocks of 5.6 and 6.0 Ms (5.8 and 5.5 Mw, respectively, according to USGS) on July 23, resulting in 1 death, 6 serious injuries, collapse of hundreds of homes and damaging kilometers of highways. Pingwu County and Beichuan County, Sichuan, also northeast of Wenchuan and close to the epicenter of a 7.2 Ms earthquake in 1976, suffered a 6.1 Ms aftershock (5.7 Mw according to USGS) on August 1; it caused 2 deaths, 345 injuries, collapse of 707 homes, damages to over 1,000 homes, and blocked 25 kilometres (16 mi) of country roads. As late as August 5, yet another aftershock of 6.1 Ms (6.2 Mw according to USGS) hit Qingchuan, Sichuan, causing 1 death, 32 injuries, telecommunication interruptions, and widespread hill slides blocking roads in the area including a national highway.\",\n",
       "  'question': 'During the May 27 aftershock, how many people were injured?',\n",
       "  'model_output': '100',\n",
       "  'ground_truth': '63'},\n",
       " {'context': \"On October 22, 2007, Apple reported quarterly revenue of US$6.22 billion, of which 30.69% came from Apple notebook sales, 19.22% from desktop sales and 26% from iPod sales. Apple's 2007 year revenue increased to US$24.01 billion with US$3.5 billion in profits. Apple ended the fiscal year 2007 with US$15.4 billion in cash and no debt.\",\n",
       "  'question': \"What was the value of Apple's cash assets at the end of 2007?\",\n",
       "  'model_output': '$17.9 billion',\n",
       "  'ground_truth': '$15.4 billion'},\n",
       " {'context': 'Experts point out that the earthquake hit an area that has been largely neglected and untouched by China\\'s economic rise. Health care is poor in inland areas such as Sichuan, highlighting the widening gap between prosperous urban dwellers and struggling rural people. Vice Minister of Health Gao Qiang told reporters in Beijing that the \"public health care system in China is insufficient.\" The Vice Minister of Health also suggested that the government would pick up the costs of care to earthquake victims, many of whom have little or no insurance: \"The government should be responsible for providing medical treatment to them,\" he said.',\n",
       "  'question': 'What did the Vice Minster of Health call the public health system in China?',\n",
       "  'model_output': 'a \"disaster\"',\n",
       "  'ground_truth': 'insufficient'},\n",
       " {'context': 'The climate has become warmer in Montana and continues to do so. The glaciers in Glacier National Park have receded and are predicted to melt away completely in a few decades. Many Montana cities set heat records during July 2007, the hottest month ever recorded in Montana. Winters are warmer, too, and have fewer cold spells. Previously these cold spells had killed off bark beetles which are now attacking the forests of western Montana. The combination of warmer weather, attack by beetles, and mismanagement during past years has led to a substantial increase in the severity of forest fires in Montana. According to a study done for the U.S. Environmental Protection Agency by the Harvard School of Engineering and Applied Science, portions of Montana will experience a 200-percent increase in area burned by wildfires, and an 80-percent increase in related air pollution.',\n",
       "  'question': 'In what year did many cities in Montana set heat records?',\n",
       "  'model_output': '2017',\n",
       "  'ground_truth': '2007'},\n",
       " {'context': 'From 1842 onwards, Chopin showed signs of serious illness. After a solo recital in Paris on 21 February 1842, he wrote to Grzymała: \"I have to lie in bed all day long, my mouth and tonsils are aching so much.\" He was forced by illness to decline a written invitation from Alkan to participate in a repeat performance of the Beethoven Seventh Symphony arrangement at Erard\\'s on 1 March 1843. Late in 1844, Charles Hallé visited Chopin and found him \"hardly able to move, bent like a half-opened penknife and evidently in great pain\", although his spirits returned when he started to play the piano for his visitor. Chopin\\'s health continued to deteriorate, particularly from this time onwards. Modern research suggests that apart from any other illnesses, he may also have suffered from temporal lobe epilepsy.',\n",
       "  'question': 'Starting in what year did Chopin start showing evidence of very bad health?',\n",
       "  'model_output': '1838',\n",
       "  'ground_truth': '1842'},\n",
       " {'context': 'Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand side technologies.',\n",
       "  'question': 'What does a passive solar technique do?',\n",
       "  'model_output': 'reduce the amount of energy needed to heat a home',\n",
       "  'ground_truth': 'reduce the need for alternate resources'},\n",
       " {'context': 'On February 6, 2016, one day before her performance at the Super Bowl, Beyoncé released a new single exclusively on music streaming service Tidal called \"Formation\".',\n",
       "  'question': 'What kind of platform was the song released?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'music streaming'},\n",
       " {'context': 'On April 4, 2008, Beyoncé married Jay Z. She publicly revealed their marriage in a video montage at the listening party for her third studio album, I Am... Sasha Fierce, in Manhattan\\'s Sony Club on October 22, 2008. I Am... Sasha Fierce was released on November 18, 2008 in the United States. The album formally introduces Beyoncé\\'s alter ego Sasha Fierce, conceived during the making of her 2003 single \"Crazy in Love\", selling 482,000 copies in its first week, debuting atop the Billboard 200, and giving Beyoncé her third consecutive number-one album in the US. The album featured the number-one song \"Single Ladies (Put a Ring on It)\" and the top-five songs \"If I Were a Boy\" and \"Halo\". Achieving the accomplishment of becoming her longest-running Hot 100 single in her career, \"Halo\"\\'s success in the US helped Beyoncé attain more top-ten singles on the list than any other woman during the 2000s. It also included the successful \"Sweet Dreams\", and singles \"Diva\", \"Ego\", \"Broken-Hearted Girl\" and \"Video Phone\". The music video for \"Single Ladies\" has been parodied and imitated around the world, spawning the \"first major dance craze\" of the Internet age according to the Toronto Star. The video has won several awards, including Best Video at the 2009 MTV Europe Music Awards, the 2009 Scottish MOBO Awards, and the 2009 BET Awards. At the 2009 MTV Video Music Awards, the video was nominated for nine awards, ultimately winning three including Video of the Year. Its failure to win the Best Female Video category, which went to American country pop singer Taylor Swift\\'s \"You Belong with Me\", led to Kanye West interrupting the ceremony and Beyoncé improvising a re-presentation of Swift\\'s award during her own acceptance speech. In March 2009, Beyoncé embarked on the I Am... World Tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $119.5 million.',\n",
       "  'question': 'Which prominent star felt the 2009 Female Video of the Year award should have went to Beyoncé instead of Taylor Swift?',\n",
       "  'model_output': 'Katy Perry',\n",
       "  'ground_truth': 'Kanye West'},\n",
       " {'context': 'The 2008 Sichuan earthquake or the Great Sichuan earthquake, measured at 8.0 Ms and 7.9 Mw, and occurred at 02:28:01 PM China Standard Time at epicenter (06:28:01 UTC) on May 12 in Sichuan province, killed 69,197 people and left 18,222 missing.',\n",
       "  'question': 'What year did the Sichuan earthquake take place?',\n",
       "  'model_output': 'May 12, 2008',\n",
       "  'ground_truth': '2008'},\n",
       " {'context': 'The novel has been noted for its poignant exploration of different forms of courage. Scout\\'s impulsive inclination to fight students who insult Atticus reflects her attempt to stand up for him and defend him. Atticus is the moral center of the novel, however, and he teaches Jem one of the most significant lessons of courage. In a statement that foreshadows Atticus\\' motivation for defending Tom Robinson and describes Mrs. Dubose, who is determined to break herself of a morphine addiction, Atticus tells Jem that courage is \"when you\\'re licked before you begin but you begin anyway and you see it through no matter what\".',\n",
       "  'question': 'Who is the moral center of the novel?',\n",
       "  'model_output': 'Atticus',\n",
       "  'ground_truth': 'Atticus'},\n",
       " {'context': \"Spectre had its world premiere in London on 26 October 2015 at the Royal Albert Hall, the same day as its general release in the United Kingdom and Republic of Ireland. Following the announcement of the start of filming, Paramount Pictures brought forward the release of Mission: Impossible – Rogue Nation to avoid competing with Spectre. In March 2015 IMAX corporation announced that Spectre would be screened in its cinemas, following Skyfall's success with the company. In the UK it received a wider release than Skyfall, with a minimum of 647 cinemas including 40 IMAX screens, compared to Skyfall's 587 locations and 21 IMAX screens.\",\n",
       "  'question': 'What movie prompted IMAX to show Spectre?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Skyfall'},\n",
       " {'context': 'Chopin\\'s original publishers included Maurice Schlesinger and Camille Pleyel. His works soon began to appear in popular 19th-century piano anthologies. The first collected edition was by Breitkopf & Härtel (1878–1902). Among modern scholarly editions of Chopin\\'s works are the version under the name of Paderewski published between 1937 and 1966 and the more recent Polish \"National Edition\", edited by Jan Ekier, both of which contain detailed explanations and discussions regarding choices and sources.',\n",
       "  'question': \"Who released the first collection of Chopin's works?\",\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Breitkopf & Härtel'},\n",
       " {'context': 'New York has architecturally noteworthy buildings in a wide range of styles and from distinct time periods, from the saltbox style Pieter Claesen Wyckoff House in Brooklyn, the oldest section of which dates to 1656, to the modern One World Trade Center, the skyscraper at Ground Zero in Lower Manhattan and currently the most expensive new office tower in the world.',\n",
       "  'question': 'In what borough is the Pieter Claesen Wyckoff House located?',\n",
       "  'model_output': 'Brooklyn',\n",
       "  'ground_truth': 'Brooklyn'},\n",
       " {'context': \"Although the Chinese government was initially praised for its response to the quake (especially in comparison to Myanmar's ruling military junta's blockade of aid during Cyclone Nargis), it then saw an erosion in confidence over the school construction scandal.\",\n",
       "  'question': 'Over what scandal did the Chinese government lose in public opinion?',\n",
       "  'model_output': 'the SARS outbreak',\n",
       "  'ground_truth': 'school construction scandal'},\n",
       " {'context': 'Besides parents, Liu Shaokun (刘绍坤), a Sichuan school teacher, was detained on June 25, 2008 for \"disseminating rumors and destroying social order\" about the Sichuan earthquake. Liu’s family was later told that he was being investigated on suspicion of the crime of inciting subversion. Liu had travelled to the Shifang, taken photos of collapsed school buildings, and put them online. He had also expressed his anger at “the shoddy tofu-dregs buildings” (豆腐渣工程) in a media interview. He was ordered to serve one year of re-education through labor (RTL). According to the organization Human Rights in China, Liu has been released to serve his RTL sentence outside of the labor camp.',\n",
       "  'question': 'Where did he call the schoolhouses shoddy?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'in a media interview'},\n",
       " {'context': 'Prior Zelda games have employed a theme of two separate, yet connected, worlds. In A Link to the Past, Link travels between a \"Light World\" and a \"Dark World\"; in Ocarina of Time, as well as in Oracle of Ages, Link travels between two different time periods. The Zelda team sought to reuse this motif in the series\\' latest installment. It was suggested that Link transform into a wolf, much like he metamorphoses into a rabbit in the Dark World of A Link to the Past.[m] The story of the game was created by Aonuma, and later underwent several changes by scenario writers Mitsuhiro Takano and Aya Kyogoku. Takano created the script for the story scenes, while Kyogoku and Takayuki Ikkaku handled the actual in-game script. Aonuma left his team working on the new idea while he directed The Minish Cap for the Game Boy Advance. When he returned, he found the Twilight Princess team struggling. Emphasis on the parallel worlds and the wolf transformation had made Link\\'s character unbelievable. Aonuma also felt the gameplay lacked the caliber of innovation found in Phantom Hourglass, which was being developed with touch controls for the Nintendo DS. At the same time, the Wii was under development with the code name \"Revolution\". Miyamoto thought that the Revolution\\'s pointing device, the Wii Remote, was well suited for aiming arrows in Zelda, and suggested that Aonuma consider using it.[n]',\n",
       "  'question': 'Who proposed that the Wii Remote be used in the control scheme for Twilight Princess?',\n",
       "  'model_output': 'Satoru Iwata',\n",
       "  'ground_truth': 'Miyamoto'},\n",
       " {'context': 'Fryderyk Chopin was born in Żelazowa Wola, 46 kilometres (29 miles) west of Warsaw, in what was then the Duchy of Warsaw, a Polish state established by Napoleon. The parish baptismal record gives his birthday as 22 February 1810, and cites his given names in the Latin form Fridericus Franciscus (in Polish, he was Fryderyk Franciszek). However, the composer and his family used the birthdate 1 March,[n 2] which is now generally accepted as the correct date.',\n",
       "  'question': \"What was the latin form of Frédéric's full name?\",\n",
       "  'model_output': 'Frédéric Chopin',\n",
       "  'ground_truth': 'Fridericus Franciscus'},\n",
       " {'context': 'Described as being \"sexy, seductive and provocative\" when performing on stage, Beyoncé has said that she originally created the alter ego \"Sasha Fierce\" to keep that stage persona separate from who she really is. She described Sasha as being \"too aggressive, too strong, too sassy [and] too sexy\", stating, \"I\\'m not like her in real life at all.\" Sasha was conceived during the making of \"Crazy in Love\", and Beyoncé introduced her with the release of her 2008 album I Am... Sasha Fierce. In February 2010, she announced in an interview with Allure magazine that she was comfortable enough with herself to no longer need Sasha Fierce. However, Beyoncé announced in May 2012 that she would bring her back for her Revel Presents: Beyoncé Live shows later that month.',\n",
       "  'question': 'How does she describe Sasha?',\n",
       "  'model_output': 'as a \"little girl\"',\n",
       "  'ground_truth': 'too aggressive, too strong'},\n",
       " {'context': 'Beyoncé Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (née Beyincé), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager. Beyoncé\\'s name is a tribute to her mother\\'s maiden name. Beyoncé\\'s younger sister Solange is also a singer and a former member of Destiny\\'s Child. Mathew is African-American, while Tina is of Louisiana Creole descent (with African, Native American, French, Cajun, and distant Irish and Spanish ancestry). Through her mother, Beyoncé is a descendant of Acadian leader Joseph Broussard. She was raised in a Methodist household.',\n",
       "  'question': 'Beyoncé was raised in what religion?',\n",
       "  'model_output': 'Baptist',\n",
       "  'ground_truth': 'Methodist'},\n",
       " {'context': 'Barbara Harff and Ted Gurr defined genocide as \"the promotion and execution of policies by a state or its agents which result in the deaths of a substantial portion of a group ...[when] the victimized groups are defined primarily in terms of their communal characteristics, i.e., ethnicity, religion or nationality.\" Harff and Gurr also differentiate between genocides and politicides by the characteristics by which members of a group are identified by the state. In genocides, the victimized groups are defined primarily in terms of their communal characteristics, i.e., ethnicity, religion or nationality. In politicides the victim groups are defined primarily in terms of their hierarchical position or political opposition to the regime and dominant groups. Daniel D. Polsby and Don B. Kates, Jr. state that \"... we follow Harff\\'s distinction between genocides and \\'pogroms,\\' which she describes as \\'short-lived outbursts by mobs, which, although often condoned by authorities, rarely persist.\\' If the violence persists for long enough, however, Harff argues, the distinction between condonation and complicity collapses.\"',\n",
       "  'question': 'What was important to Harff and Gurr to distinguish from genocides?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'politicides'},\n",
       " {'context': 'Since October 2004, the iPod line has dominated digital music player sales in the United States, with over 90% of the market for hard drive-based players and over 70% of the market for all types of players. During the year from January 2004 to January 2005, the high rate of sales caused its U.S. market share to increase from 31% to 65% and in July 2005, this market share was measured at 74%. In January 2007 the iPod market share reached 72.7% according to Bloomberg Online.',\n",
       "  'question': \"Who reported Apple's market share in 2007?\",\n",
       "  'model_output': 'IDC',\n",
       "  'ground_truth': 'Bloomberg Online'},\n",
       " {'context': 'Twilight Princess takes place several centuries after Ocarina of Time and Majora\\'s Mask, and begins with a youth named Link who is working as a ranch hand in Ordon Village. One day, the village is attacked by Bulblins, who carry off the village\\'s children with Link in pursuit before he encounters a wall of Twilight. A Shadow Beast pulls him beyond the wall into the Realm of Twilight, where he is transformed into a wolf and imprisoned. Link is soon freed by an imp-like Twilight being named Midna, who dislikes Link but agrees to help him if he obeys her unconditionally. She guides him to Princess Zelda. Zelda explains that Zant, the King of the Twilight, has stolen the light from three of the four Light Spirits and conquered Hyrule. In order to save Hyrule, Link must first restore the Light Spirits by entering the Twilight-covered areas and, as a wolf, recover the Spirits\\' lost light. He must do this by collecting the multiple \"Tears of Light\"; once all the Tears of Light are collected for one area, he restores that area\\'s Light Spirit. As he restores them, the Light Spirits return Link to his Hylian form.',\n",
       "  'question': 'Where is Link working as a ranch hand?',\n",
       "  'model_output': 'Hyrule Ranch',\n",
       "  'ground_truth': 'Ordon Village'},\n",
       " {'context': 'In January 2013, Destiny\\'s Child released Love Songs, a compilation album of the romance-themed songs from their previous albums and a newly recorded track, \"Nuclear\". Beyoncé performed the American national anthem singing along with a pre-recorded track at President Obama\\'s second inauguration in Washington, D.C. The following month, Beyoncé performed at the Super Bowl XLVII halftime show, held at the Mercedes-Benz Superdome in New Orleans. The performance stands as the second most tweeted about moment in history at 268,000 tweets per minute. At the 55th Annual Grammy Awards, Beyoncé won for Best Traditional R&B Performance for \"Love on Top\". Her feature-length documentary film, Life Is But a Dream, first aired on HBO on February 16, 2013. The film, which she directed and produced herself, featured footage from her childhood, her as a mother and businesswoman, recording, rehearsing for live performances, and her return to the spotlight following Blue Ivy\\'s birth. Its DVD release in November 2013 was accompanied by footage from the Revel Presents: Beyoncé Live concerts and a new song, \"God Made You Beautiful\". In February 2013, Beyoncé signed a global publishing agreement with Warner/Chappell Music, which would cover her future songwriting and then-upcoming studio album.',\n",
       "  'question': 'When did Destiny\\'s Child release its album \"Love Songs\"?',\n",
       "  'model_output': 'The 2019-2024 Outlook for Canned and Fro',\n",
       "  'ground_truth': 'January 2013'},\n",
       " {'context': 'The successful outcome of antimicrobial therapy with antibacterial compounds depends on several factors. These include host defense mechanisms, the location of infection, and the pharmacokinetic and pharmacodynamic properties of the antibacterial. A bactericidal activity of antibacterials may depend on the bacterial growth phase, and it often requires ongoing metabolic activity and division of bacterial cells. These findings are based on laboratory studies, and in clinical settings have also been shown to eliminate bacterial infection. Since the activity of antibacterials depends frequently on its concentration, in vitro characterization of antibacterial activity commonly includes the determination of the minimum inhibitory concentration and minimum bactericidal concentration of an antibacterial. To predict clinical outcome, the antimicrobial activity of an antibacterial is usually combined with its pharmacokinetic profile, and several pharmacological parameters are used as markers of drug efficacy.',\n",
       "  'question': 'What does a successful treatment using antibiotics entail?',\n",
       "  'model_output': 'a. a single dose of antibiotics b. a single dose of antib',\n",
       "  'ground_truth': 'host defense mechanisms, the location of infection, and the pharmacokinetic and pharmacodynamic properties of the antibacterial'},\n",
       " {'context': 'Chopin also endowed popular dance forms with a greater range of melody and expression. Chopin\\'s mazurkas, while originating in the traditional Polish dance (the mazurek), differed from the traditional variety in that they were written for the concert hall rather than the dance hall; \"it was Chopin who put the mazurka on the European musical map.\" The series of seven polonaises published in his lifetime (another nine were published posthumously), beginning with the Op. 26 pair (published 1836), set a new standard for music in the form. His waltzes were also written specifically for the salon recital rather than the ballroom and are frequently at rather faster tempos than their dance-floor equivalents.',\n",
       "  'question': 'What did Chopin add to the modern dance of his era?',\n",
       "  'model_output': 'a new style of piano playing',\n",
       "  'ground_truth': 'greater range of melody and expression.'},\n",
       " {'context': \"The story sees Bond pitted against the global criminal organisation Spectre, marking the group's first appearance in an Eon Productions film since 1971's Diamonds Are Forever,[N 2] and tying Craig's series of films together with an overarching storyline. Several recurring James Bond characters, including M, Q and Eve Moneypenny return, with the new additions of Léa Seydoux as Dr. Madeleine Swann, Dave Bautista as Mr. Hinx, Andrew Scott as Max Denbigh and Monica Bellucci as Lucia Sciarra.\",\n",
       "  'question': 'Which recurring James Bond characters appear in Spectre?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'M, Q and Eve Moneypenny'},\n",
       " {'context': 'The two mature piano sonatas (No. 2, Op. 35, written in 1839 and No. 3, Op. 58, written in 1844) are in four movements. In Op. 35, Chopin was able to combine within a formal large musical structure many elements of his virtuosic piano technique—\"a kind of dialogue between the public pianism of the brilliant style and the German sonata principle\". The last movement, a brief (75-bar) perpetuum mobile in which the hands play in unmodified octave unison throughout, was found shocking and unmusical by contemporaries, including Schumann. The Op. 58 sonata is closer to the German tradition, including many passages of complex counterpoint, \"worthy of Brahms\" according to the music historians Kornel Michałowski and Jim Samson.',\n",
       "  'question': 'Which movement was found lacking in musicality by Schumann?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'The last movement'},\n",
       " {'context': 'I Love New York (stylized I ❤ NY) is both a logo and a song that are the basis of an advertising campaign and have been used since 1977 to promote tourism in New York City, and later to promote New York State as well. The trademarked logo, owned by New York State Empire State Development, appears in souvenir shops and brochures throughout the city and state, some licensed, many not. The song is the state song of New York.',\n",
       "  'question': 'When was I ❤ NY first used in advertisements?',\n",
       "  'model_output': '1977',\n",
       "  'ground_truth': '1977'},\n",
       " {'context': 'Beyoncé has received praise for her stage presence and voice during live performances. Jarett Wieselman of the New York Post placed her at number one on her list of the Five Best Singer/Dancers. According to Barbara Ellen of The Guardian Beyoncé is the most in-charge female artist she\\'s seen onstage, while Alice Jones of The Independent wrote she \"takes her role as entertainer so seriously she\\'s almost too good.\" The ex-President of Def Jam L.A. Reid has described Beyoncé as the greatest entertainer alive. Jim Farber of the Daily News and Stephanie Classen of Star Phoenix both praised her strong voice and her stage presence.',\n",
       "  'question': 'For what does Beyonce receive praise?',\n",
       "  'model_output': 'her vocal ability',\n",
       "  'ground_truth': 'stage presence'},\n",
       " {'context': 'Beyoncé\\'s second solo album B\\'Day was released on September 5, 2006, in the US, to coincide with her twenty-fifth birthday. It sold 541,000 copies in its first week and debuted atop the Billboard 200, becoming Beyoncé\\'s second consecutive number-one album in the United States. The album\\'s lead single \"Déjà Vu\", featuring Jay Z, reached the top five on the Billboard Hot 100 chart. The second international single \"Irreplaceable\" was a commercial success worldwide, reaching number one in Australia, Hungary, Ireland, New Zealand and the United States. B\\'Day also produced three other singles; \"Ring the Alarm\", \"Get Me Bodied\", and \"Green Light\" (released in the United Kingdom only).',\n",
       "  'question': \"How many copies did B'Day sell during the first week of its release?\",\n",
       "  'model_output': '1.1 million',\n",
       "  'ground_truth': '541,000'},\n",
       " {'context': 'The first phase of Eddy Street Commons, a $215 million development located adjacent to the University of Notre Dame campus and funded by the university, broke ground on June 3, 2008. The Eddy Street Commons drew union protests when workers hired by the City of South Bend to construct the public parking garage picketed the private work site after a contractor hired non-union workers. The developer, Kite Realty out of Indianapolis, has made agreements with major national chains rather than local businesses, a move that has led to criticism from alumni and students.',\n",
       "  'question': 'Which entity did Notre Dame hire to build a parking structure outside of Eddy Street Commons?',\n",
       "  'model_output': 'The South Bend Parking Authority',\n",
       "  'ground_truth': 'the City of South Bend'},\n",
       " {'context': 'In 1817 the Saxon Palace was requisitioned by Warsaw\\'s Russian governor for military use, and the Warsaw Lyceum was reestablished in the Kazimierz Palace (today the rectorate of Warsaw University). Fryderyk and his family moved to a building, which still survives, adjacent to the Kazimierz Palace. During this period, Fryderyk was sometimes invited to the Belweder Palace as playmate to the son of the ruler of Russian Poland, Grand Duke Constantine; he played the piano for the Duke and composed a march for him. Julian Ursyn Niemcewicz, in his dramatic eclogue, \"Nasze Przebiegi\" (\"Our Discourses\", 1818), attested to \"little Chopin\\'s\" popularity.',\n",
       "  'question': 'The Warsaw Lyceum was moved to where?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Kazimierz Palace'},\n",
       " {'context': \"Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus run by the Congregation of Holy Cross. The current Basilica of the Sacred Heart is located on the spot of Fr. Sorin's original church, which became too small for the growing college. It is built in French Revival style and it is decorated by stained glass windows imported directly from France. The interior was painted by Luigi Gregori, an Italian painter invited by Fr. Sorin to be artist in residence. The Basilica also features a bell tower with a carillon. Inside the church there are also sculptures by Ivan Mestrovic. The Grotto of Our Lady of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France. It is very popular among students and alumni as a place of prayer and meditation, and it is considered one of the most beloved spots on campus.\",\n",
       "  'question': 'In which architectural style is the Basilica of the Sacred Heart at Notre Dame made?',\n",
       "  'model_output': 'Gothic Revival',\n",
       "  'ground_truth': 'French Revival'},\n",
       " {'context': \"During installation, an iPod is associated with one host computer. Each time an iPod connects to its host computer, iTunes can synchronize entire music libraries or music playlists either automatically or manually. Song ratings can be set on an iPod and synchronized later to the iTunes library, and vice versa. A user can access, play, and add music on a second computer if an iPod is set to manual and not automatic sync, but anything added or edited will be reversed upon connecting and syncing with the main computer and its library. If a user wishes to automatically sync music with another computer, an iPod's library will be entirely wiped and replaced with the other computer's library.\",\n",
       "  'question': 'What will be erased if an iPod is set to sync with a different computer?',\n",
       "  'model_output': 'The 2018-19 school year is off to a great start!',\n",
       "  'ground_truth': 'library'},\n",
       " {'context': 'Beyoncé further expanded her acting career, starring as blues singer Etta James in the 2008 musical biopic, Cadillac Records. Her performance in the film received praise from critics, and she garnered several nominations for her portrayal of James, including a Satellite Award nomination for Best Supporting Actress, and a NAACP Image Award nomination for Outstanding Supporting Actress. Beyoncé donated her entire salary from the film to Phoenix House, an organization of rehabilitation centers for heroin addicts around the country. On January 20, 2009, Beyoncé performed James\\' \"At Last\" at the First Couple\\'s first inaugural ball. Beyoncé starred opposite Ali Larter and Idris Elba in the thriller, Obsessed. She played Sharon Charles, a mother and wife who learns of a woman\\'s obsessive behavior over her husband. Although the film received negative reviews from critics, the movie did well at the US box office, grossing $68 million—$60 million more than Cadillac Records—on a budget of $20 million. The fight scene finale between Sharon and the character played by Ali Larter also won the 2010 MTV Movie Award for Best Fight.',\n",
       "  'question': 'Beyonce portrayed which character in the film, Cadillac Records?',\n",
       "  'model_output': 'Etta James',\n",
       "  'ground_truth': 'Etta James'},\n",
       " {'context': 'With his mazurkas and polonaises, Chopin has been credited with introducing to music a new sense of nationalism. Schumann, in his 1836 review of the piano concertos, highlighted the composer\\'s strong feelings for his native Poland, writing that \"Now that the Poles are in deep mourning [after the failure of the November 1830 rising], their appeal to us artists is even stronger ... If the mighty autocrat in the north [i.e. Nicholas I of Russia] could know that in Chopin\\'s works, in the simple strains of his mazurkas, there lurks a dangerous enemy, he would place a ban on his music. Chopin\\'s works are cannon buried in flowers!\" The biography of Chopin published in 1863 under the name of Franz Liszt (but probably written by Carolyne zu Sayn-Wittgenstein) claims that Chopin \"must be ranked first among the first musicians ... individualizing in themselves the poetic sense of an entire nation.\"',\n",
       "  'question': \"In his review Schumann made note of Chopin's emotions for what?\",\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Poland'},\n",
       " {'context': \"Montana has a larger Native American population numerically and percentage-wise than most U.S. states. Although the state ranked 45th in population (according to the 2010 U.S. Census), it ranked 19th in total native people population. Native people constituted 6.5 percent of the state's total population, the sixth highest percentage of all 50 states. Montana has three counties in which Native Americans are a majority: Big Horn, Glacier, and Roosevelt. Other counties with large Native American populations include Blaine, Cascade, Hill, Missoula, and Yellowstone counties. The state's Native American population grew by 27.9 percent between 1980 and 1990 (at a time when Montana's entire population rose just 1.6 percent), and by 18.5 percent between 2000 and 2010. As of 2009, almost two-thirds of Native Americans in the state live in urban areas. Of Montana's 20 largest cities, Polson (15.7 percent), Havre (13.0 percent), Great Falls (5.0 percent), Billings (4.4 percent), and Anaconda (3.1 percent) had the greatest percentage of Native American residents in 2010. Billings (4,619), Great Falls (2,942), Missoula (1,838), Havre (1,210), and Polson (706) have the most Native Americans living there. The state's seven reservations include more than twelve distinct Native American ethnolinguistic groups.\",\n",
       "  'question': 'What three counties are Native Americans a majority?',\n",
       "  'model_output': 'Apache, Navajo, and Pueblo',\n",
       "  'ground_truth': 'Big Horn, Glacier, and Roosevelt'},\n",
       " {'context': 'In April, during the Revolution of 1848 in Paris, he left for London, where he performed at several concerts and at numerous receptions in great houses. This tour was suggested to him by his Scottish pupil Jane Stirling and her elder sister. Stirling also made all the logistical arrangements and provided much of the necessary funding.',\n",
       "  'question': 'What two people suggested the 1848 tour?',\n",
       "  'model_output': 'The 2018-2019 school year is off to a great',\n",
       "  'ground_truth': 'Jane Stirling and her elder sister'},\n",
       " {'context': \"Universal Music Group decided not to renew their contract with the iTunes Store on July 3, 2007. Universal will now supply iTunes in an 'at will' capacity.\",\n",
       "  'question': 'What entity did Universal have a contract with prior to July of 2007?',\n",
       "  'model_output': 'EMI',\n",
       "  'ground_truth': 'iTunes Store'},\n",
       " {'context': \"All of the highways into Wenchuan, and others throughout the province, were damaged, resulting in delayed arrival of the rescue troops. In Beichuan County, 80% of the buildings collapsed according to Xinhua News. In the city of Shifang, the collapse of two chemical plants led to leakage of some 80 tons of liquid ammonia, with hundreds of people reported buried. In the city of Dujiangyan, south-east of the epicenter, a whole school collapsed with 900 students buried and fewer than 60 survived. The Juyuan Middle School, where many teenagers were buried, was excavated by civilians and cranes. Dujiangyan is home of the Dujiangyan Irrigation System, an ancient water diversion project which is still in use and is a UNESCO World Heritage Site. The project's famous Fish Mouth was cracked but not severely damaged otherwise.\",\n",
       "  'question': 'What highways into Wenchuan  were damaged?',\n",
       "  'model_output': 'The 2018-19 school year is off to a great start!',\n",
       "  'ground_truth': 'All of the highways'},\n",
       " {'context': \"The topography of the state is roughly defined by the Continental Divide, which splits much of the state into distinct eastern and western regions. Most of Montana's 100 or more named mountain ranges are concentrated in the western half of the state, most of which is geologically and geographically part of the Northern Rocky Mountains. The Absaroka and Beartooth ranges in the south-central part of the state are technically part of the Central Rocky Mountains. The Rocky Mountain Front is a significant feature in the north-central portion of the state, and there are a number of isolated island ranges that interrupt the prairie landscape common in the central and eastern parts of the state. About 60 percent of the state is prairie, part of the northern Great Plains.\",\n",
       "  'question': 'How much of the state is prarie?',\n",
       "  'model_output': '25%',\n",
       "  'ground_truth': 'About 60 percent'},\n",
       " {'context': 'In The New Yorker music critic Jody Rosen described Beyoncé as \"the most important and compelling popular musician of the twenty-first century..... the result, the logical end point, of a century-plus of pop.\" When The Guardian named her Artist of the Decade, Llewyn-Smith wrote, \"Why Beyoncé? [...] Because she made not one but two of the decade\\'s greatest singles, with Crazy in Love and Single Ladies (Put a Ring on It), not to mention her hits with Destiny\\'s Child; and this was the decade when singles – particularly R&B singles – regained their status as pop\\'s favourite medium. [...] [She] and not any superannuated rock star was arguably the greatest live performer of the past 10 years.\" In 2013, Beyoncé made the Time 100 list, Baz Luhrmann writing \"no one has that voice, no one moves the way she moves, no one can hold an audience the way she does... When Beyoncé does an album, when Beyoncé sings a song, when Beyoncé does anything, it\\'s an event, and it\\'s broadly influential. Right now, she is the heir-apparent diva of the USA — the reigning national voice.\" In 2014, Beyoncé was listed again on the Time 100 and also featured on the cover of the issue.',\n",
       "  'question': 'When did Beyonce first make the Time 100 List?',\n",
       "  'model_output': '2013',\n",
       "  'ground_truth': '2013'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16716716716716717"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('outputs2.json', 'w') as f:\n",
    "    json.dump(testing_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 out = generate(LLamaModel, tokenizer, question, IST=get_IST(context))                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'generate'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 out = generate(LLamaModel, tokenizer, question, IST=get_IST(context))                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'generate'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = generate(LLamaModel, tokenizer, question, IST=get_IST(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56bf76ef3aeaaa14008c9667',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé attended St. Mary\\'s Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. Beyoncé\\'s interest in music and performing continued after winning a school talent show at age seven, singing John Lennon\\'s \"Imagine\" to beat 15/16-year-olds. In fall of 1990, Beyoncé enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school\\'s choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. Beyoncé was also a member of the choir at St. John\\'s United Methodist Church as a soloist for two years.',\n",
       " 'question': 'Which song did Beyonce sing to win a competition at age 7?',\n",
       " 'answers': {'text': ['Imagine'], 'answer_start': [385]}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['test'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lit_llama import model\n",
    "import random\n",
    "from lit_llama import LLaMA, Tokenizer\n",
    "from lit_llama.utils import EmptyInitOnDevice, lazy_load, llama_model_lookup\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fabric = L.Fabric(devices=1)\n",
    "tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/andrew/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The BeyoncÃ© Experience'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "squad = squad.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('datasets/alpaca_data_cleaned.json') as f:\n",
    "    alpaca_json = json.load(f)\n",
    "\n",
    "# Create tokenized j\n",
    "alpaca_json_tokens = []\n",
    "\n",
    "for item in squad['train']:\n",
    "    alpaca_json_tokens.append(\n",
    "        {\n",
    "            'instruction': tokenizer.encode(item['context'], bos=True, eos=False, device=fabric.device),\n",
    "            'input': tokenizer.encode(item['question'], bos=False, eos=False, device=fabric.device),\n",
    "            'output':tokenizer.encode(item['answers']['text'][0], bos=False, eos=True, device=fabric.device)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_example():\n",
    "    index = random.sample(range(len(alpaca_json_tokens)), k=1)[0]\n",
    "    # IST\n",
    "    IST = IST_generator(LLamaModel(alpaca_json_tokens[index]['instruction'].unsqueeze(0).to(fabric.device))[1])[:,-1,:]\n",
    "\n",
    "    # Question\n",
    "    question = LLamaModel.transformer.wte(alpaca_json_tokens[index]['input'].unsqueeze(0).to(fabric.device)).squeeze()\n",
    "\n",
    "    # Answer fragment\n",
    "    answer_len = alpaca_json_tokens[index]['output'].size(0)\n",
    "    trunc_len = random.randint(0,answer_len-1)\n",
    "    #print(answer_len)\n",
    "    #print(trunc_len)\n",
    "\n",
    "    truncated_answer = alpaca_json_tokens[index]['output'][:trunc_len]\n",
    "    truncated_answer = LLamaModel.transformer.wte(truncated_answer)\n",
    "    \n",
    "    target_tokens = torch.cat([alpaca_json_tokens[index]['input'], alpaca_json_tokens[index]['output'][:trunc_len+1]])\n",
    "    #print(tokenizer.decode(target_tokens))\n",
    "\n",
    "    llama_input = torch.cat([IST,question,truncated_answer])\n",
    "    return llama_input.unsqueeze(0), target_tokens.type(torch.LongTensor).unsqueeze(0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "IST_generator = model.Block(LLaMA_config)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(IST_generator.parameters(), lr=1e-4)\n",
    "IST_generator = IST_generator.to(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 23.68 GiB total capacity; 22.74 GiB already allocated; 57.50 MiB free; 22.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m batch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m32\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     \u001b[39minput\u001b[39m, target \u001b[39m=\u001b[39m get_single_example()\n\u001b[1;32m     13\u001b[0m     llama_output \u001b[39m=\u001b[39m LLamaModel\u001b[39m.\u001b[39mforward_embeddings(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mbfloat16))[\u001b[39m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(llama_output\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(fabric\u001b[39m.\u001b[39mdevice), target\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(fabric\u001b[39m.\u001b[39mdevice))\n",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m, in \u001b[0;36mget_single_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(alpaca_json_tokens)), k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[39m# IST\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m IST \u001b[39m=\u001b[39m IST_generator(LLamaModel(alpaca_json_tokens[index][\u001b[39m'\u001b[39;49m\u001b[39minstruction\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(fabric\u001b[39m.\u001b[39;49mdevice))[\u001b[39m1\u001b[39;49m])[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\n\u001b[1;32m      6\u001b[0m \u001b[39m# Question\u001b[39;00m\n\u001b[1;32m      7\u001b[0m question \u001b[39m=\u001b[39m LLamaModel\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mwte(alpaca_json_tokens[index][\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(fabric\u001b[39m.\u001b[39mdevice))\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/lit-llama/lit_llama/model.py:138\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    137\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrms_1(x))\n\u001b[0;32m--> 138\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrms_2(x))\n\u001b[1;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/lit-llama/lit_llama/model.py:229\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 229\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49msilu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc1(x)) \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc2(x)\n\u001b[1;32m    230\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(x)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 23.68 GiB total capacity; 22.74 GiB already allocated; 57.50 MiB free; 22.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for param in LLamaModel.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "for epoch in range(1000):\n",
    "    #batch = get_batch(LLamaModel.transformer.wte, batch_size=1)\n",
    "    \n",
    "    batch_loss = 0\n",
    "\n",
    "    for i in range(32):\n",
    "        input, target = get_single_example()\n",
    "        llama_output = LLamaModel.forward_embeddings(input.type(torch.bfloat16))[0]\n",
    "        loss = loss_fn(llama_output.squeeze().to(fabric.device), target.squeeze().to(fabric.device))\n",
    "        batch_loss += loss\n",
    "    batch_loss /= 32\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(IST_generator.mlp.c_fc1.weight)\n",
    "    losses.append(batch_loss.item())\n",
    "    print(f'epoch {epoch}, loss={batch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path: Path = Path(\"checkpoints/lit-llama/7B/lit-llama.pth\")\n",
    "tokenizer_path: Path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "\n",
    "\n",
    "def load_LLaMA(checkpoint_path):\n",
    "    with lazy_load(checkpoint_path) as checkpoint:\n",
    "        name = llama_model_lookup(checkpoint)\n",
    "\n",
    "        with EmptyInitOnDevice(\n",
    "                device=fabric.device, dtype=dtype, quantization_mode=None # We won't quantize the weights\n",
    "        ):\n",
    "            model = LLaMA.from_name(name)\n",
    "\n",
    "        model.load_state_dict(checkpoint)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Finished loading the first model\n",
      "Finished loading models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtype = torch.bfloat16 if fabric.device.type == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "\n",
    "LLaMA_config = model.LLaMAConfig.from_name('7B')\n",
    "print('Loading models...')\n",
    "# Load the LLaMa model and the IST generator (also a LLaMA model)\n",
    "LLamaModel = load_LLaMA(checkpoint_path).to(fabric.device)\n",
    "#LLamaModel = LLaMA(LLaMA_config).to(fabric.device)\n",
    "print('Finished loading the first model')\n",
    "print('Finished loading models')\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "IST_schemes = ['vanilla', 'last 4', '2nd to last', 'all layers']\n",
    "scheme_losses = {}\n",
    "\n",
    "IST_generator = model.Block(LLaMA_config)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(IST_generator.parameters(), lr=1e-4)\n",
    "IST_generator = IST_generator.to(fabric.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in LLamaModel.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IST(string):\n",
    "    tokens = tokenizer.encode(string).unsqueeze(0).type(torch.LongTensor).to(fabric.device)\n",
    "    x = LLamaModel(tokens)[1]\n",
    "    x = IST_generator(x)\n",
    "    return x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, IST=None, max_new_tokens=200):\n",
    "  \n",
    "    \n",
    "    generated = ''\n",
    "    tokenized_input = tokenizer.encode(prompt).to(fabric.device)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            last_logits = model(tokenized_input.unsqueeze(0), IST.type(torch.bfloat16))[0][:,-1,:]\n",
    "            new_token = torch.argmax(last_logits, dim=1)\n",
    "            if(new_token == 2): #eos\n",
    "                break\n",
    "            generated += tokenizer.decode(new_token)\n",
    "            tokenized_input = torch.cat([tokenized_input, new_token])\n",
    "\n",
    "    print(tokenizer.decode(tokenized_input))\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = squad['train'][0]['context']\n",
    "question = squad['train'][0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did Beyonce call her first concert tour? The BeyoncÃ© Experience\n"
     ]
    }
   ],
   "source": [
    "out = generate(LLamaModel, tokenizer, question, IST=get_IST(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56d65c671c850414009470bd',\n",
       " 'title': '2008_Sichuan_earthquake',\n",
       " 'context': 'Besides parents, Liu Shaokun (åç»å¤), a Sichuan school teacher, was detained on June 25, 2008 for \"disseminating rumors and destroying social order\" about the Sichuan earthquake. Liuâs family was later told that he was being investigated on suspicion of the crime of inciting subversion. Liu had travelled to the Shifang, taken photos of collapsed school buildings, and put them online. He had also expressed his anger at âthe shoddy tofu-dregs buildingsâ (è±èæ¸£å·¥ç¨) in a media interview. He was ordered to serve one year of re-education through labor (RTL). According to the organization Human Rights in China, Liu has been released to serve his RTL sentence outside of the labor camp.',\n",
       " 'question': 'What was his assigned punishment ?',\n",
       " 'answers': {'text': ['one year of re-education'], 'answer_start': [508]}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
